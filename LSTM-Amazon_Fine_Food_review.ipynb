{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/\n",
    "# LSTM for sequence classification in the IMDB dataset\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout, Dense \n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copied from T-SNE assignment \n",
    "#using the SQLite Table to read data. \n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "con = sqlite3.connect('database.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = pd.read_sql_query(\"\"\" SELECT  * FROM Reviews LIMIT 50000\"\"\",con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Review_text=filtered_data['Text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/47091490/4084039\n",
    "import re\n",
    "\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/sebleier/554280\n",
    "# we are removing the words from the stop words list: 'no', 'nor', 'not'\n",
    "stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "            'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [00:17<00:00, 2799.26it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "Clean_Review_text = []\n",
    "# tqdm is for printing the status bar\n",
    "for sentance in tqdm(Review_text):\n",
    "    sent = decontracted(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = sent.replace('br', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "    Clean_Review_text.append(sent.lower().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "never life tasted good babka crazy good real babka gram mother use make\n"
     ]
    }
   ],
   "source": [
    "print(Clean_Review_text[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data['Text']=Clean_Review_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Vocabolary of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "for sentence in Clean_Review_text:\n",
    "    words = sentence.split()\n",
    "    vocab += words\n",
    "    \n",
    "#print(Review_text.shape,\"Sentences are Present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating frequency of each word in vocabolary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40113 Unique Words present in Vocabolary \n"
     ]
    }
   ],
   "source": [
    "#https://stackabuse.com/introduction-to-pythons-collections-module/\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "words = vocab\n",
    "\n",
    "counts = Counter(vocab)\n",
    "\n",
    "print(len(counts.most_common()),\"Unique Words present in Vocabolary \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting The Sorted frequency Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackabuse.com/introduction-to-pythons-collections-module/\n",
    "#https://docs.python.org/3/library/itertools.html\n",
    "\n",
    "from itertools import islice\n",
    "from collections import Counter\n",
    "\n",
    "vocab_size = len(Counter(words).most_common()) + 1\n",
    "\n",
    "top_words_count = 5000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40114\n"
     ]
    }
   ],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words and  their frequencies :\n",
      "[('not', 55345), ('like', 22921), ('good', 17808), ('taste', 15562), ('one', 15250), ('great', 14684), ('would', 14662), ('product', 14459), ('coffee', 13210), ('flavor', 12975), ('tea', 12262), ('love', 11141), ('food', 9533), ('get', 9478), ('no', 9377), ('amazon', 9034), ('really', 8905), ('much', 8202), ('time', 7412), ('also', 7390), ('use', 7195), ('little', 7192), ('buy', 6755), ('tried', 6729), ('best', 6711)]\n",
      "Words and their index :\n",
      "[('not', 1), ('like', 2), ('good', 3), ('taste', 4), ('one', 5), ('great', 6), ('would', 7), ('product', 8), ('coffee', 9), ('flavor', 10), ('tea', 11), ('love', 12), ('food', 13), ('get', 14), ('no', 15), ('amazon', 16), ('really', 17), ('much', 18), ('time', 19), ('also', 20), ('use', 21), ('little', 22), ('buy', 23), ('tried', 24), ('best', 25)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sorted_words = counts.most_common(5000)\n",
    "\n",
    "\n",
    "word_index_buffer = dict()\n",
    "\n",
    "i = 1\n",
    "for word,frequency in sorted_words:\n",
    "        word_index_buffer[word] = i\n",
    "        i += 1\n",
    "    \n",
    "print(\"Words and  their frequencies :\")\n",
    "print(sorted_words[:25])\n",
    "\n",
    "\n",
    "print(\"Words and their index :\")\n",
    "print(list(islice(word_index_buffer.items(), 25)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Indexing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Word_Indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>[48, 217, 0, 480, 33, 13, 120, 44, 3, 96, 8, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>[8, 264, 1907, 4726, 1827, 889, 889, 159, 105,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>[4344, 184, 0, 236, 0, 1806, 2992, 375, 228, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>[148, 2433, 495, 0, 392, 44, 75, 726, 1382, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>[6, 3390, 6, 27, 2122, 2518, 457, 3390, 569, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  bought several vitality canned dog food produc...   \n",
       "1      Not as Advertised  product arrived labeled jumbo salted peanuts p...   \n",
       "2  \"Delight\" says it all  confection around centuries light pillowy citr...   \n",
       "3         Cough Medicine  looking secret ingredient robitussin believe f...   \n",
       "4            Great taffy  great taffy great price wide assortment yummy ...   \n",
       "\n",
       "                                        Word_Indices  \n",
       "0  [48, 217, 0, 480, 33, 13, 120, 44, 3, 96, 8, 5...  \n",
       "1  [8, 264, 1907, 4726, 1827, 889, 889, 159, 105,...  \n",
       "2  [4344, 184, 0, 236, 0, 1806, 2992, 375, 228, 0...  \n",
       "3  [148, 2433, 495, 0, 392, 44, 75, 726, 1382, 12...  \n",
       "4  [6, 3390, 6, 27, 2122, 2518, 457, 3390, 569, 3...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#credits :https://github.com/sai977\n",
    "#filtered_data = filtered_data.drop(columns=['index'])\n",
    "\n",
    "def apply_text_index(row):  \n",
    "    holder = []\n",
    "    for word in row['Text'].split():\n",
    "        if word in word_index_buffer:\n",
    "            holder.append(word_index_buffer[word]) \n",
    "        else:\n",
    "            holder.append(0)        \n",
    "          \n",
    "    return holder\n",
    "\n",
    "\n",
    "filtered_data['Word_Indices'] = filtered_data.apply(lambda row: apply_text_index(row),axis=1)\n",
    "filtered_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_x(x):\n",
    "    if x>=3:\n",
    "        x=1\n",
    "    else:\n",
    "        x=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Word_Indices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>bought several vitality canned dog food produc...</td>\n",
       "      <td>[48, 217, 0, 480, 33, 13, 120, 44, 3, 96, 8, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>product arrived labeled jumbo salted peanuts p...</td>\n",
       "      <td>[8, 264, 1907, 4726, 1827, 889, 889, 159, 105,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>confection around centuries light pillowy citr...</td>\n",
       "      <td>[4344, 184, 0, 236, 0, 1806, 2992, 375, 228, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>looking secret ingredient robitussin believe f...</td>\n",
       "      <td>[148, 2433, 495, 0, 392, 44, 75, 726, 1382, 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>great taffy great price wide assortment yummy ...</td>\n",
       "      <td>[6, 3390, 6, 27, 2122, 2518, 457, 3390, 569, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      1  1303862400   \n",
       "1                     0                       0      0  1346976000   \n",
       "2                     1                       1      1  1219017600   \n",
       "3                     3                       3      0  1307923200   \n",
       "4                     0                       0      1  1350777600   \n",
       "\n",
       "                 Summary                                               Text  \\\n",
       "0  Good Quality Dog Food  bought several vitality canned dog food produc...   \n",
       "1      Not as Advertised  product arrived labeled jumbo salted peanuts p...   \n",
       "2  \"Delight\" says it all  confection around centuries light pillowy citr...   \n",
       "3         Cough Medicine  looking secret ingredient robitussin believe f...   \n",
       "4            Great taffy  great taffy great price wide assortment yummy ...   \n",
       "\n",
       "                                        Word_Indices  \n",
       "0  [48, 217, 0, 480, 33, 13, 120, 44, 3, 96, 8, 5...  \n",
       "1  [8, 264, 1907, 4726, 1827, 889, 889, 159, 105,...  \n",
       "2  [4344, 184, 0, 236, 0, 1806, 2992, 375, 228, 0...  \n",
       "3  [148, 2433, 495, 0, 392, 44, 75, 726, 1382, 12...  \n",
       "4  [6, 3390, 6, 27, 2122, 2518, 457, 3390, 569, 3...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data['Score'] = filtered_data['Score'].map(lambda x : check_x(x))\n",
    "filtered_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(filtered_data['Word_Indices'].values,filtered_data['Score'],test_size=0.3,shuffle=False,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number words present in 1st review: 19\n",
      "List of word index present in first review:\n",
      "[8, 264, 1907, 4726, 1827, 889, 889, 159, 105, 885, 3335, 1, 114, 2963, 1580, 2236, 0, 8, 4726]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number words present in 1st review:\",len(x_train[1]))\n",
    "\n",
    "print(\"List of word index present in first review:\")\n",
    "print(x_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number words present in 1st review: 500\n",
      "List of word index present in first review:\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    8  264 1907 4726 1827  889  889  159  105\n",
      "  885 3335    1  114 2963 1580 2236    0    8 4726]\n"
     ]
    }
   ],
   "source": [
    "#https://keras.io/preprocessing/sequence/\n",
    "\n",
    "max_review_length = 500\n",
    "\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)\n",
    "\n",
    "print(\"Number words present in 1st review:\",len(x_train[1]))\n",
    "\n",
    "print(\"List of word index present in first review:\")\n",
    "print(x_train[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: 1 Layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.regularizers import L1L2\n",
    "reg = L1L2(0.01, 0.01)\n",
    "embedding_vector_length = 32\n",
    "batch_size =500\n",
    "epochs=12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credits :https://github.com/sai977\n",
    "# Plot train and cross validation loss\n",
    "def plot_train_cv_loss(trained_model, epochs, colors=['b']):\n",
    "    fig, ax = plt.subplots(1,1)    \n",
    "    ax.set_xlabel('epoch') \n",
    "    ax.set_ylabel('Categorical Crossentropy Loss')\n",
    "    x_axis_values = list(range(1,epochs+1))\n",
    "\n",
    "    validation_loss = trained_model.history['val_loss']\n",
    "    train_loss = trained_model.history['loss']   \n",
    "    \n",
    "    ax.plot(x_axis_values, validation_loss, 'b', label=\"Validation Loss\")\n",
    "    ax.plot(x_axis_values, train_loss, 'r', label=\"Train Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 500, 32)           1283648   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,336,949\n",
      "Trainable params: 1,336,949\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.20))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/12\n",
      "35000/35000 [==============================] - ETA: 6:45 - loss: 0.6933 - acc: 0.466 - ETA: 5:30 - loss: 0.6909 - acc: 0.655 - ETA: 4:45 - loss: 0.6880 - acc: 0.730 - ETA: 4:23 - loss: 0.6853 - acc: 0.761 - ETA: 4:26 - loss: 0.6821 - acc: 0.782 - ETA: 4:19 - loss: 0.6786 - acc: 0.794 - ETA: 4:05 - loss: 0.6739 - acc: 0.808 - ETA: 3:57 - loss: 0.6692 - acc: 0.813 - ETA: 3:46 - loss: 0.6632 - acc: 0.819 - ETA: 3:37 - loss: 0.6563 - acc: 0.822 - ETA: 3:34 - loss: 0.6463 - acc: 0.826 - ETA: 3:28 - loss: 0.6336 - acc: 0.829 - ETA: 3:21 - loss: 0.6179 - acc: 0.830 - ETA: 3:15 - loss: 0.6057 - acc: 0.833 - ETA: 3:09 - loss: 0.6050 - acc: 0.832 - ETA: 3:14 - loss: 0.5959 - acc: 0.833 - ETA: 3:09 - loss: 0.5883 - acc: 0.832 - ETA: 3:04 - loss: 0.5780 - acc: 0.834 - ETA: 3:00 - loss: 0.5704 - acc: 0.835 - ETA: 2:56 - loss: 0.5634 - acc: 0.836 - ETA: 2:52 - loss: 0.5582 - acc: 0.836 - ETA: 2:47 - loss: 0.5536 - acc: 0.836 - ETA: 2:42 - loss: 0.5475 - acc: 0.838 - ETA: 2:38 - loss: 0.5421 - acc: 0.838 - ETA: 2:34 - loss: 0.5371 - acc: 0.839 - ETA: 2:32 - loss: 0.5322 - acc: 0.840 - ETA: 2:28 - loss: 0.5289 - acc: 0.839 - ETA: 2:24 - loss: 0.5266 - acc: 0.839 - ETA: 2:20 - loss: 0.5218 - acc: 0.840 - ETA: 2:16 - loss: 0.5173 - acc: 0.841 - ETA: 2:13 - loss: 0.5134 - acc: 0.841 - ETA: 2:09 - loss: 0.5105 - acc: 0.841 - ETA: 2:05 - loss: 0.5078 - acc: 0.842 - ETA: 2:01 - loss: 0.5058 - acc: 0.842 - ETA: 1:59 - loss: 0.5026 - acc: 0.842 - ETA: 1:56 - loss: 0.4999 - acc: 0.843 - ETA: 1:52 - loss: 0.4971 - acc: 0.843 - ETA: 1:49 - loss: 0.4941 - acc: 0.843 - ETA: 1:45 - loss: 0.4922 - acc: 0.843 - ETA: 1:42 - loss: 0.4900 - acc: 0.844 - ETA: 1:38 - loss: 0.4880 - acc: 0.844 - ETA: 1:35 - loss: 0.4870 - acc: 0.843 - ETA: 1:31 - loss: 0.4852 - acc: 0.844 - ETA: 1:27 - loss: 0.4837 - acc: 0.843 - ETA: 1:24 - loss: 0.4825 - acc: 0.843 - ETA: 1:20 - loss: 0.4806 - acc: 0.843 - ETA: 1:16 - loss: 0.4793 - acc: 0.843 - ETA: 1:13 - loss: 0.4774 - acc: 0.844 - ETA: 1:09 - loss: 0.4761 - acc: 0.844 - ETA: 1:06 - loss: 0.4741 - acc: 0.844 - ETA: 1:02 - loss: 0.4721 - acc: 0.844 - ETA: 59s - loss: 0.4711 - acc: 0.844 - ETA: 55s - loss: 0.4699 - acc: 0.84 - ETA: 52s - loss: 0.4684 - acc: 0.84 - ETA: 49s - loss: 0.4668 - acc: 0.84 - ETA: 45s - loss: 0.4658 - acc: 0.84 - ETA: 42s - loss: 0.4648 - acc: 0.84 - ETA: 39s - loss: 0.4640 - acc: 0.84 - ETA: 35s - loss: 0.4624 - acc: 0.84 - ETA: 32s - loss: 0.4610 - acc: 0.84 - ETA: 29s - loss: 0.4591 - acc: 0.84 - ETA: 25s - loss: 0.4579 - acc: 0.84 - ETA: 22s - loss: 0.4564 - acc: 0.84 - ETA: 19s - loss: 0.4544 - acc: 0.84 - ETA: 16s - loss: 0.4528 - acc: 0.84 - ETA: 12s - loss: 0.4511 - acc: 0.84 - ETA: 9s - loss: 0.4506 - acc: 0.8454 - ETA: 6s - loss: 0.4486 - acc: 0.845 - ETA: 3s - loss: 0.4470 - acc: 0.845 - 255s 7ms/step - loss: 0.4452 - acc: 0.8463 - val_loss: 0.3438 - val_acc: 0.8451\n",
      "Epoch 2/12\n",
      "35000/35000 [==============================] - ETA: 3:15 - loss: 0.3239 - acc: 0.848 - ETA: 3:06 - loss: 0.3198 - acc: 0.850 - ETA: 3:22 - loss: 0.3059 - acc: 0.860 - ETA: 3:13 - loss: 0.3116 - acc: 0.856 - ETA: 3:07 - loss: 0.3116 - acc: 0.857 - ETA: 3:02 - loss: 0.3107 - acc: 0.859 - ETA: 2:58 - loss: 0.3065 - acc: 0.861 - ETA: 2:54 - loss: 0.3027 - acc: 0.864 - ETA: 2:56 - loss: 0.3043 - acc: 0.863 - ETA: 2:52 - loss: 0.3027 - acc: 0.862 - ETA: 2:49 - loss: 0.2990 - acc: 0.863 - ETA: 2:45 - loss: 0.2975 - acc: 0.864 - ETA: 2:41 - loss: 0.2943 - acc: 0.866 - ETA: 2:42 - loss: 0.2890 - acc: 0.869 - ETA: 2:39 - loss: 0.2848 - acc: 0.872 - ETA: 2:36 - loss: 0.2823 - acc: 0.873 - ETA: 2:32 - loss: 0.2794 - acc: 0.875 - ETA: 2:29 - loss: 0.2787 - acc: 0.876 - ETA: 2:25 - loss: 0.2778 - acc: 0.876 - ETA: 2:28 - loss: 0.2774 - acc: 0.877 - ETA: 2:31 - loss: 0.2771 - acc: 0.877 - ETA: 2:28 - loss: 0.2771 - acc: 0.876 - ETA: 2:25 - loss: 0.2756 - acc: 0.878 - ETA: 2:23 - loss: 0.2749 - acc: 0.878 - ETA: 2:19 - loss: 0.2736 - acc: 0.878 - ETA: 2:16 - loss: 0.2745 - acc: 0.878 - ETA: 2:12 - loss: 0.2753 - acc: 0.878 - ETA: 2:08 - loss: 0.2760 - acc: 0.879 - ETA: 2:06 - loss: 0.2781 - acc: 0.878 - ETA: 2:02 - loss: 0.2801 - acc: 0.877 - ETA: 1:59 - loss: 0.2816 - acc: 0.877 - ETA: 1:55 - loss: 0.2827 - acc: 0.876 - ETA: 1:52 - loss: 0.2829 - acc: 0.877 - ETA: 1:48 - loss: 0.2832 - acc: 0.876 - ETA: 1:46 - loss: 0.2824 - acc: 0.877 - ETA: 1:43 - loss: 0.2805 - acc: 0.878 - ETA: 1:39 - loss: 0.2799 - acc: 0.878 - ETA: 1:36 - loss: 0.2782 - acc: 0.879 - ETA: 1:33 - loss: 0.2773 - acc: 0.879 - ETA: 1:30 - loss: 0.2777 - acc: 0.880 - ETA: 1:27 - loss: 0.2766 - acc: 0.880 - ETA: 1:24 - loss: 0.2757 - acc: 0.881 - ETA: 1:21 - loss: 0.2757 - acc: 0.881 - ETA: 1:18 - loss: 0.2744 - acc: 0.882 - ETA: 1:14 - loss: 0.2738 - acc: 0.882 - ETA: 1:12 - loss: 0.2730 - acc: 0.882 - ETA: 1:09 - loss: 0.2717 - acc: 0.883 - ETA: 1:05 - loss: 0.2709 - acc: 0.883 - ETA: 1:02 - loss: 0.2707 - acc: 0.883 - ETA: 59s - loss: 0.2702 - acc: 0.884 - ETA: 56s - loss: 0.2692 - acc: 0.88 - ETA: 53s - loss: 0.2697 - acc: 0.88 - ETA: 50s - loss: 0.2690 - acc: 0.88 - ETA: 47s - loss: 0.2680 - acc: 0.88 - ETA: 44s - loss: 0.2673 - acc: 0.88 - ETA: 41s - loss: 0.2665 - acc: 0.88 - ETA: 38s - loss: 0.2668 - acc: 0.88 - ETA: 35s - loss: 0.2664 - acc: 0.88 - ETA: 32s - loss: 0.2660 - acc: 0.88 - ETA: 29s - loss: 0.2655 - acc: 0.88 - ETA: 26s - loss: 0.2644 - acc: 0.88 - ETA: 23s - loss: 0.2641 - acc: 0.88 - ETA: 20s - loss: 0.2640 - acc: 0.88 - ETA: 17s - loss: 0.2627 - acc: 0.88 - ETA: 14s - loss: 0.2617 - acc: 0.88 - ETA: 11s - loss: 0.2618 - acc: 0.88 - ETA: 8s - loss: 0.2611 - acc: 0.8887 - ETA: 5s - loss: 0.2604 - acc: 0.889 - ETA: 2s - loss: 0.2597 - acc: 0.889 - 236s 7ms/step - loss: 0.2594 - acc: 0.8898 - val_loss: 0.2543 - val_acc: 0.8950\n",
      "Epoch 3/12\n",
      "35000/35000 [==============================] - ETA: 3:12 - loss: 0.2081 - acc: 0.902 - ETA: 3:06 - loss: 0.1923 - acc: 0.915 - ETA: 3:02 - loss: 0.2058 - acc: 0.913 - ETA: 2:58 - loss: 0.2091 - acc: 0.913 - ETA: 3:08 - loss: 0.2083 - acc: 0.914 - ETA: 3:03 - loss: 0.2087 - acc: 0.914 - ETA: 2:59 - loss: 0.2055 - acc: 0.917 - ETA: 2:55 - loss: 0.2018 - acc: 0.920 - ETA: 2:51 - loss: 0.2033 - acc: 0.918 - ETA: 2:53 - loss: 0.2007 - acc: 0.918 - ETA: 2:49 - loss: 0.1991 - acc: 0.918 - ETA: 2:45 - loss: 0.1983 - acc: 0.919 - ETA: 2:42 - loss: 0.1985 - acc: 0.918 - ETA: 2:38 - loss: 0.2007 - acc: 0.916 - ETA: 2:35 - loss: 0.2010 - acc: 0.916 - ETA: 2:35 - loss: 0.1992 - acc: 0.917 - ETA: 2:32 - loss: 0.2009 - acc: 0.916 - ETA: 2:30 - loss: 0.1996 - acc: 0.917 - ETA: 2:26 - loss: 0.2021 - acc: 0.916 - ETA: 2:23 - loss: 0.2027 - acc: 0.915 - ETA: 2:22 - loss: 0.2011 - acc: 0.916 - ETA: 2:19 - loss: 0.2019 - acc: 0.916 - ETA: 2:15 - loss: 0.2025 - acc: 0.916 - ETA: 2:12 - loss: 0.2023 - acc: 0.916 - ETA: 2:10 - loss: 0.2021 - acc: 0.917 - ETA: 2:07 - loss: 0.2032 - acc: 0.916 - ETA: 2:05 - loss: 0.2038 - acc: 0.915 - ETA: 2:02 - loss: 0.2028 - acc: 0.916 - ETA: 1:59 - loss: 0.2017 - acc: 0.917 - ETA: 1:56 - loss: 0.2017 - acc: 0.917 - ETA: 1:53 - loss: 0.2032 - acc: 0.916 - ETA: 1:51 - loss: 0.2031 - acc: 0.916 - ETA: 1:48 - loss: 0.2031 - acc: 0.916 - ETA: 1:45 - loss: 0.2024 - acc: 0.916 - ETA: 1:42 - loss: 0.2026 - acc: 0.916 - ETA: 1:39 - loss: 0.2021 - acc: 0.917 - ETA: 1:37 - loss: 0.2017 - acc: 0.917 - ETA: 1:33 - loss: 0.2012 - acc: 0.917 - ETA: 1:30 - loss: 0.2012 - acc: 0.917 - ETA: 1:27 - loss: 0.2009 - acc: 0.918 - ETA: 1:24 - loss: 0.1997 - acc: 0.918 - ETA: 1:21 - loss: 0.2000 - acc: 0.918 - ETA: 1:19 - loss: 0.2004 - acc: 0.918 - ETA: 1:16 - loss: 0.2010 - acc: 0.918 - ETA: 1:14 - loss: 0.2009 - acc: 0.918 - ETA: 1:11 - loss: 0.2017 - acc: 0.918 - ETA: 1:08 - loss: 0.2019 - acc: 0.918 - ETA: 1:05 - loss: 0.2018 - acc: 0.918 - ETA: 1:02 - loss: 0.2019 - acc: 0.918 - ETA: 59s - loss: 0.2024 - acc: 0.917 - ETA: 56s - loss: 0.2023 - acc: 0.91 - ETA: 53s - loss: 0.2027 - acc: 0.91 - ETA: 50s - loss: 0.2029 - acc: 0.91 - ETA: 47s - loss: 0.2028 - acc: 0.91 - ETA: 44s - loss: 0.2021 - acc: 0.91 - ETA: 41s - loss: 0.2028 - acc: 0.91 - ETA: 38s - loss: 0.2023 - acc: 0.91 - ETA: 35s - loss: 0.2021 - acc: 0.91 - ETA: 32s - loss: 0.2023 - acc: 0.91 - ETA: 29s - loss: 0.2016 - acc: 0.91 - ETA: 26s - loss: 0.2009 - acc: 0.91 - ETA: 23s - loss: 0.2003 - acc: 0.91 - ETA: 20s - loss: 0.1997 - acc: 0.91 - ETA: 17s - loss: 0.1997 - acc: 0.91 - ETA: 14s - loss: 0.1995 - acc: 0.91 - ETA: 11s - loss: 0.1994 - acc: 0.91 - ETA: 8s - loss: 0.1994 - acc: 0.9188 - ETA: 5s - loss: 0.1989 - acc: 0.919 - ETA: 2s - loss: 0.1992 - acc: 0.918 - 234s 7ms/step - loss: 0.1988 - acc: 0.9191 - val_loss: 0.2496 - val_acc: 0.9025\n",
      "Epoch 4/12\n",
      "35000/35000 [==============================] - ETA: 4:38 - loss: 0.1800 - acc: 0.932 - ETA: 4:10 - loss: 0.1598 - acc: 0.936 - ETA: 4:02 - loss: 0.1501 - acc: 0.941 - ETA: 3:48 - loss: 0.1559 - acc: 0.937 - ETA: 3:39 - loss: 0.1568 - acc: 0.936 - ETA: 3:39 - loss: 0.1667 - acc: 0.931 - ETA: 3:29 - loss: 0.1654 - acc: 0.931 - ETA: 3:22 - loss: 0.1646 - acc: 0.932 - ETA: 3:16 - loss: 0.1612 - acc: 0.934 - ETA: 3:10 - loss: 0.1620 - acc: 0.935 - ETA: 3:09 - loss: 0.1612 - acc: 0.936 - ETA: 3:04 - loss: 0.1661 - acc: 0.934 - ETA: 2:58 - loss: 0.1666 - acc: 0.935 - ETA: 2:54 - loss: 0.1645 - acc: 0.935 - ETA: 2:50 - loss: 0.1645 - acc: 0.935 - ETA: 2:46 - loss: 0.1639 - acc: 0.936 - ETA: 2:45 - loss: 0.1642 - acc: 0.936 - ETA: 2:40 - loss: 0.1639 - acc: 0.936 - ETA: 2:36 - loss: 0.1648 - acc: 0.935 - ETA: 2:32 - loss: 0.1656 - acc: 0.935 - ETA: 2:29 - loss: 0.1654 - acc: 0.935 - ETA: 2:27 - loss: 0.1659 - acc: 0.934 - ETA: 2:23 - loss: 0.1661 - acc: 0.934 - ETA: 2:19 - loss: 0.1654 - acc: 0.935 - ETA: 2:16 - loss: 0.1658 - acc: 0.934 - ETA: 2:13 - loss: 0.1651 - acc: 0.934 - ETA: 2:10 - loss: 0.1662 - acc: 0.934 - ETA: 2:07 - loss: 0.1655 - acc: 0.934 - ETA: 2:04 - loss: 0.1668 - acc: 0.934 - ETA: 2:00 - loss: 0.1671 - acc: 0.933 - ETA: 1:57 - loss: 0.1676 - acc: 0.933 - ETA: 1:53 - loss: 0.1686 - acc: 0.933 - ETA: 1:51 - loss: 0.1680 - acc: 0.934 - ETA: 1:48 - loss: 0.1692 - acc: 0.934 - ETA: 1:45 - loss: 0.1693 - acc: 0.933 - ETA: 1:41 - loss: 0.1702 - acc: 0.933 - ETA: 1:38 - loss: 0.1701 - acc: 0.933 - ETA: 1:35 - loss: 0.1704 - acc: 0.933 - ETA: 1:32 - loss: 0.1710 - acc: 0.933 - ETA: 1:29 - loss: 0.1701 - acc: 0.933 - ETA: 1:26 - loss: 0.1700 - acc: 0.933 - ETA: 1:23 - loss: 0.1700 - acc: 0.933 - ETA: 1:20 - loss: 0.1698 - acc: 0.933 - ETA: 1:17 - loss: 0.1702 - acc: 0.933 - ETA: 1:14 - loss: 0.1697 - acc: 0.933 - ETA: 1:11 - loss: 0.1699 - acc: 0.933 - ETA: 1:08 - loss: 0.1707 - acc: 0.933 - ETA: 1:05 - loss: 0.1713 - acc: 0.933 - ETA: 1:02 - loss: 0.1718 - acc: 0.933 - ETA: 59s - loss: 0.1721 - acc: 0.933 - ETA: 56s - loss: 0.1716 - acc: 0.93 - ETA: 53s - loss: 0.1722 - acc: 0.93 - ETA: 50s - loss: 0.1718 - acc: 0.93 - ETA: 47s - loss: 0.1713 - acc: 0.93 - ETA: 44s - loss: 0.1711 - acc: 0.93 - ETA: 41s - loss: 0.1718 - acc: 0.93 - ETA: 38s - loss: 0.1717 - acc: 0.93 - ETA: 35s - loss: 0.1717 - acc: 0.93 - ETA: 32s - loss: 0.1715 - acc: 0.93 - ETA: 29s - loss: 0.1711 - acc: 0.93 - ETA: 26s - loss: 0.1721 - acc: 0.93 - ETA: 23s - loss: 0.1724 - acc: 0.93 - ETA: 20s - loss: 0.1726 - acc: 0.93 - ETA: 17s - loss: 0.1731 - acc: 0.93 - ETA: 15s - loss: 0.1733 - acc: 0.93 - ETA: 12s - loss: 0.1735 - acc: 0.93 - ETA: 9s - loss: 0.1738 - acc: 0.9327 - ETA: 6s - loss: 0.1739 - acc: 0.932 - ETA: 3s - loss: 0.1739 - acc: 0.932 - 237s 7ms/step - loss: 0.1743 - acc: 0.9323 - val_loss: 0.2410 - val_acc: 0.9024\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 3:13 - loss: 0.1650 - acc: 0.940 - ETA: 3:33 - loss: 0.1588 - acc: 0.943 - ETA: 3:21 - loss: 0.1599 - acc: 0.938 - ETA: 3:13 - loss: 0.1551 - acc: 0.938 - ETA: 3:06 - loss: 0.1564 - acc: 0.942 - ETA: 3:02 - loss: 0.1580 - acc: 0.941 - ETA: 2:58 - loss: 0.1587 - acc: 0.940 - ETA: 3:01 - loss: 0.1604 - acc: 0.939 - ETA: 2:57 - loss: 0.1594 - acc: 0.939 - ETA: 2:53 - loss: 0.1592 - acc: 0.940 - ETA: 2:49 - loss: 0.1603 - acc: 0.939 - ETA: 2:45 - loss: 0.1599 - acc: 0.940 - ETA: 2:46 - loss: 0.1584 - acc: 0.940 - ETA: 2:42 - loss: 0.1571 - acc: 0.941 - ETA: 2:39 - loss: 0.1571 - acc: 0.941 - ETA: 2:35 - loss: 0.1559 - acc: 0.942 - ETA: 2:32 - loss: 0.1555 - acc: 0.941 - ETA: 2:29 - loss: 0.1550 - acc: 0.942 - ETA: 2:28 - loss: 0.1559 - acc: 0.942 - ETA: 2:25 - loss: 0.1553 - acc: 0.942 - ETA: 2:21 - loss: 0.1529 - acc: 0.943 - ETA: 2:19 - loss: 0.1541 - acc: 0.943 - ETA: 2:15 - loss: 0.1538 - acc: 0.943 - ETA: 2:14 - loss: 0.1541 - acc: 0.942 - ETA: 2:10 - loss: 0.1543 - acc: 0.942 - ETA: 2:07 - loss: 0.1538 - acc: 0.942 - ETA: 2:04 - loss: 0.1529 - acc: 0.942 - ETA: 2:01 - loss: 0.1523 - acc: 0.942 - ETA: 1:58 - loss: 0.1535 - acc: 0.942 - ETA: 1:56 - loss: 0.1538 - acc: 0.941 - ETA: 1:53 - loss: 0.1536 - acc: 0.942 - ETA: 1:50 - loss: 0.1549 - acc: 0.941 - ETA: 1:47 - loss: 0.1545 - acc: 0.941 - ETA: 1:44 - loss: 0.1541 - acc: 0.941 - ETA: 1:42 - loss: 0.1539 - acc: 0.941 - ETA: 1:39 - loss: 0.1539 - acc: 0.940 - ETA: 1:36 - loss: 0.1532 - acc: 0.941 - ETA: 1:32 - loss: 0.1536 - acc: 0.941 - ETA: 1:29 - loss: 0.1536 - acc: 0.940 - ETA: 1:26 - loss: 0.1549 - acc: 0.940 - ETA: 1:24 - loss: 0.1554 - acc: 0.940 - ETA: 1:21 - loss: 0.1551 - acc: 0.940 - ETA: 1:18 - loss: 0.1555 - acc: 0.940 - ETA: 1:15 - loss: 0.1552 - acc: 0.940 - ETA: 1:12 - loss: 0.1553 - acc: 0.940 - ETA: 1:09 - loss: 0.1553 - acc: 0.940 - ETA: 1:06 - loss: 0.1562 - acc: 0.939 - ETA: 1:03 - loss: 0.1565 - acc: 0.939 - ETA: 1:00 - loss: 0.1568 - acc: 0.939 - ETA: 57s - loss: 0.1570 - acc: 0.939 - ETA: 54s - loss: 0.1571 - acc: 0.93 - ETA: 52s - loss: 0.1569 - acc: 0.93 - ETA: 49s - loss: 0.1577 - acc: 0.93 - ETA: 46s - loss: 0.1580 - acc: 0.93 - ETA: 43s - loss: 0.1583 - acc: 0.93 - ETA: 40s - loss: 0.1585 - acc: 0.93 - ETA: 37s - loss: 0.1588 - acc: 0.93 - ETA: 34s - loss: 0.1588 - acc: 0.93 - ETA: 31s - loss: 0.1594 - acc: 0.93 - ETA: 28s - loss: 0.1595 - acc: 0.93 - ETA: 26s - loss: 0.1598 - acc: 0.93 - ETA: 23s - loss: 0.1598 - acc: 0.93 - ETA: 20s - loss: 0.1598 - acc: 0.93 - ETA: 17s - loss: 0.1594 - acc: 0.93 - ETA: 14s - loss: 0.1598 - acc: 0.93 - ETA: 11s - loss: 0.1598 - acc: 0.93 - ETA: 8s - loss: 0.1606 - acc: 0.9381 - ETA: 5s - loss: 0.1608 - acc: 0.938 - ETA: 2s - loss: 0.1604 - acc: 0.938 - 231s 7ms/step - loss: 0.1610 - acc: 0.9381 - val_loss: 0.2516 - val_acc: 0.8948\n",
      "Epoch 6/12\n",
      "35000/35000 [==============================] - ETA: 3:05 - loss: 0.1463 - acc: 0.946 - ETA: 3:03 - loss: 0.1541 - acc: 0.933 - ETA: 2:59 - loss: 0.1500 - acc: 0.937 - ETA: 2:57 - loss: 0.1552 - acc: 0.937 - ETA: 3:06 - loss: 0.1485 - acc: 0.941 - ETA: 3:02 - loss: 0.1460 - acc: 0.941 - ETA: 2:57 - loss: 0.1436 - acc: 0.943 - ETA: 2:53 - loss: 0.1431 - acc: 0.943 - ETA: 2:50 - loss: 0.1460 - acc: 0.943 - ETA: 2:47 - loss: 0.1459 - acc: 0.944 - ETA: 2:54 - loss: 0.1435 - acc: 0.945 - ETA: 2:50 - loss: 0.1442 - acc: 0.944 - ETA: 2:46 - loss: 0.1446 - acc: 0.944 - ETA: 2:42 - loss: 0.1448 - acc: 0.943 - ETA: 2:38 - loss: 0.1422 - acc: 0.944 - ETA: 2:38 - loss: 0.1413 - acc: 0.945 - ETA: 2:34 - loss: 0.1417 - acc: 0.944 - ETA: 2:31 - loss: 0.1440 - acc: 0.944 - ETA: 2:27 - loss: 0.1436 - acc: 0.945 - ETA: 2:24 - loss: 0.1452 - acc: 0.944 - ETA: 2:21 - loss: 0.1458 - acc: 0.943 - ETA: 2:19 - loss: 0.1471 - acc: 0.943 - ETA: 2:16 - loss: 0.1493 - acc: 0.942 - ETA: 2:13 - loss: 0.1482 - acc: 0.943 - ETA: 2:09 - loss: 0.1490 - acc: 0.942 - ETA: 2:06 - loss: 0.1488 - acc: 0.942 - ETA: 2:04 - loss: 0.1495 - acc: 0.942 - ETA: 2:01 - loss: 0.1489 - acc: 0.943 - ETA: 1:58 - loss: 0.1488 - acc: 0.943 - ETA: 1:55 - loss: 0.1477 - acc: 0.943 - ETA: 1:52 - loss: 0.1473 - acc: 0.943 - ETA: 1:49 - loss: 0.1472 - acc: 0.943 - ETA: 1:47 - loss: 0.1485 - acc: 0.943 - ETA: 1:44 - loss: 0.1480 - acc: 0.943 - ETA: 1:41 - loss: 0.1485 - acc: 0.943 - ETA: 1:38 - loss: 0.1489 - acc: 0.942 - ETA: 1:35 - loss: 0.1485 - acc: 0.943 - ETA: 1:32 - loss: 0.1477 - acc: 0.943 - ETA: 1:29 - loss: 0.1471 - acc: 0.943 - ETA: 1:26 - loss: 0.1476 - acc: 0.943 - ETA: 1:23 - loss: 0.1469 - acc: 0.943 - ETA: 1:20 - loss: 0.1467 - acc: 0.943 - ETA: 1:17 - loss: 0.1469 - acc: 0.943 - ETA: 1:15 - loss: 0.1468 - acc: 0.943 - ETA: 1:12 - loss: 0.1474 - acc: 0.943 - ETA: 1:09 - loss: 0.1479 - acc: 0.943 - ETA: 1:06 - loss: 0.1491 - acc: 0.942 - ETA: 1:03 - loss: 0.1489 - acc: 0.942 - ETA: 1:01 - loss: 0.1487 - acc: 0.942 - ETA: 58s - loss: 0.1492 - acc: 0.942 - ETA: 55s - loss: 0.1493 - acc: 0.94 - ETA: 52s - loss: 0.1496 - acc: 0.94 - ETA: 49s - loss: 0.1497 - acc: 0.94 - ETA: 46s - loss: 0.1495 - acc: 0.94 - ETA: 43s - loss: 0.1495 - acc: 0.94 - ETA: 40s - loss: 0.1494 - acc: 0.94 - ETA: 37s - loss: 0.1494 - acc: 0.94 - ETA: 34s - loss: 0.1498 - acc: 0.94 - ETA: 31s - loss: 0.1502 - acc: 0.94 - ETA: 29s - loss: 0.1509 - acc: 0.94 - ETA: 26s - loss: 0.1508 - acc: 0.94 - ETA: 23s - loss: 0.1514 - acc: 0.94 - ETA: 20s - loss: 0.1511 - acc: 0.94 - ETA: 17s - loss: 0.1510 - acc: 0.94 - ETA: 14s - loss: 0.1511 - acc: 0.94 - ETA: 11s - loss: 0.1513 - acc: 0.94 - ETA: 8s - loss: 0.1516 - acc: 0.9423 - ETA: 5s - loss: 0.1518 - acc: 0.942 - ETA: 2s - loss: 0.1523 - acc: 0.942 - 230s 7ms/step - loss: 0.1522 - acc: 0.9419 - val_loss: 0.2544 - val_acc: 0.8981\n",
      "Epoch 7/12\n",
      "35000/35000 [==============================] - ETA: 3:12 - loss: 0.1198 - acc: 0.950 - ETA: 3:05 - loss: 0.1470 - acc: 0.940 - ETA: 3:22 - loss: 0.1549 - acc: 0.936 - ETA: 3:14 - loss: 0.1481 - acc: 0.942 - ETA: 3:07 - loss: 0.1466 - acc: 0.940 - ETA: 3:02 - loss: 0.1399 - acc: 0.943 - ETA: 2:58 - loss: 0.1320 - acc: 0.947 - ETA: 2:54 - loss: 0.1306 - acc: 0.948 - ETA: 2:57 - loss: 0.1303 - acc: 0.949 - ETA: 2:53 - loss: 0.1272 - acc: 0.950 - ETA: 2:52 - loss: 0.1267 - acc: 0.951 - ETA: 2:49 - loss: 0.1271 - acc: 0.951 - ETA: 2:45 - loss: 0.1280 - acc: 0.952 - ETA: 2:45 - loss: 0.1290 - acc: 0.952 - ETA: 2:41 - loss: 0.1301 - acc: 0.952 - ETA: 2:38 - loss: 0.1314 - acc: 0.951 - ETA: 2:34 - loss: 0.1326 - acc: 0.951 - ETA: 2:30 - loss: 0.1342 - acc: 0.950 - ETA: 2:27 - loss: 0.1331 - acc: 0.951 - ETA: 2:26 - loss: 0.1325 - acc: 0.951 - ETA: 2:23 - loss: 0.1323 - acc: 0.951 - ETA: 2:19 - loss: 0.1318 - acc: 0.951 - ETA: 2:17 - loss: 0.1311 - acc: 0.951 - ETA: 2:14 - loss: 0.1315 - acc: 0.951 - ETA: 2:12 - loss: 0.1329 - acc: 0.950 - ETA: 2:09 - loss: 0.1332 - acc: 0.950 - ETA: 2:06 - loss: 0.1342 - acc: 0.949 - ETA: 2:02 - loss: 0.1365 - acc: 0.949 - ETA: 1:59 - loss: 0.1362 - acc: 0.948 - ETA: 1:56 - loss: 0.1359 - acc: 0.948 - ETA: 1:54 - loss: 0.1374 - acc: 0.947 - ETA: 1:51 - loss: 0.1375 - acc: 0.947 - ETA: 1:48 - loss: 0.1377 - acc: 0.947 - ETA: 1:45 - loss: 0.1395 - acc: 0.946 - ETA: 1:45 - loss: 0.1399 - acc: 0.946 - ETA: 1:42 - loss: 0.1392 - acc: 0.946 - ETA: 1:38 - loss: 0.1391 - acc: 0.946 - ETA: 1:35 - loss: 0.1385 - acc: 0.947 - ETA: 1:32 - loss: 0.1388 - acc: 0.947 - ETA: 1:29 - loss: 0.1398 - acc: 0.946 - ETA: 1:26 - loss: 0.1394 - acc: 0.946 - ETA: 1:23 - loss: 0.1397 - acc: 0.946 - ETA: 1:20 - loss: 0.1409 - acc: 0.945 - ETA: 1:17 - loss: 0.1415 - acc: 0.945 - ETA: 1:14 - loss: 0.1412 - acc: 0.945 - ETA: 1:11 - loss: 0.1418 - acc: 0.946 - ETA: 1:08 - loss: 0.1415 - acc: 0.946 - ETA: 1:05 - loss: 0.1414 - acc: 0.946 - ETA: 1:02 - loss: 0.1410 - acc: 0.946 - ETA: 59s - loss: 0.1424 - acc: 0.946 - ETA: 56s - loss: 0.1424 - acc: 0.94 - ETA: 53s - loss: 0.1425 - acc: 0.94 - ETA: 50s - loss: 0.1426 - acc: 0.94 - ETA: 47s - loss: 0.1424 - acc: 0.94 - ETA: 44s - loss: 0.1424 - acc: 0.94 - ETA: 41s - loss: 0.1425 - acc: 0.94 - ETA: 38s - loss: 0.1421 - acc: 0.94 - ETA: 35s - loss: 0.1423 - acc: 0.94 - ETA: 32s - loss: 0.1428 - acc: 0.94 - ETA: 29s - loss: 0.1428 - acc: 0.94 - ETA: 26s - loss: 0.1431 - acc: 0.94 - ETA: 23s - loss: 0.1429 - acc: 0.94 - ETA: 20s - loss: 0.1428 - acc: 0.94 - ETA: 17s - loss: 0.1432 - acc: 0.94 - ETA: 14s - loss: 0.1432 - acc: 0.94 - ETA: 11s - loss: 0.1433 - acc: 0.94 - ETA: 8s - loss: 0.1441 - acc: 0.9455 - ETA: 5s - loss: 0.1444 - acc: 0.945 - ETA: 2s - loss: 0.1445 - acc: 0.945 - 234s 7ms/step - loss: 0.1446 - acc: 0.9454 - val_loss: 0.2653 - val_acc: 0.8951\n",
      "Epoch 8/12\n",
      "35000/35000 [==============================] - ETA: 3:15 - loss: 0.1383 - acc: 0.956 - ETA: 3:08 - loss: 0.1197 - acc: 0.965 - ETA: 3:04 - loss: 0.1254 - acc: 0.962 - ETA: 3:01 - loss: 0.1340 - acc: 0.957 - ETA: 3:02 - loss: 0.1316 - acc: 0.958 - ETA: 3:04 - loss: 0.1257 - acc: 0.960 - ETA: 2:59 - loss: 0.1244 - acc: 0.958 - ETA: 2:59 - loss: 0.1271 - acc: 0.956 - ETA: 3:02 - loss: 0.1269 - acc: 0.956 - ETA: 3:11 - loss: 0.1300 - acc: 0.954 - ETA: 3:08 - loss: 0.1288 - acc: 0.956 - ETA: 3:03 - loss: 0.1298 - acc: 0.955 - ETA: 3:05 - loss: 0.1321 - acc: 0.955 - ETA: 3:08 - loss: 0.1320 - acc: 0.954 - ETA: 3:02 - loss: 0.1315 - acc: 0.955 - ETA: 2:57 - loss: 0.1305 - acc: 0.956 - ETA: 2:52 - loss: 0.1310 - acc: 0.955 - ETA: 2:48 - loss: 0.1312 - acc: 0.955 - ETA: 2:43 - loss: 0.1324 - acc: 0.954 - ETA: 2:41 - loss: 0.1331 - acc: 0.954 - ETA: 2:37 - loss: 0.1320 - acc: 0.954 - ETA: 2:33 - loss: 0.1304 - acc: 0.955 - ETA: 2:29 - loss: 0.1315 - acc: 0.954 - ETA: 2:25 - loss: 0.1313 - acc: 0.954 - ETA: 2:23 - loss: 0.1317 - acc: 0.954 - ETA: 2:19 - loss: 0.1315 - acc: 0.954 - ETA: 2:15 - loss: 0.1326 - acc: 0.954 - ETA: 2:12 - loss: 0.1330 - acc: 0.954 - ETA: 2:08 - loss: 0.1336 - acc: 0.953 - ETA: 2:04 - loss: 0.1333 - acc: 0.953 - ETA: 2:02 - loss: 0.1330 - acc: 0.953 - ETA: 1:58 - loss: 0.1329 - acc: 0.953 - ETA: 1:55 - loss: 0.1334 - acc: 0.953 - ETA: 1:52 - loss: 0.1331 - acc: 0.953 - ETA: 1:48 - loss: 0.1324 - acc: 0.953 - ETA: 1:46 - loss: 0.1328 - acc: 0.953 - ETA: 1:43 - loss: 0.1329 - acc: 0.953 - ETA: 1:39 - loss: 0.1328 - acc: 0.953 - ETA: 1:36 - loss: 0.1327 - acc: 0.953 - ETA: 1:32 - loss: 0.1326 - acc: 0.953 - ETA: 1:30 - loss: 0.1329 - acc: 0.953 - ETA: 1:26 - loss: 0.1328 - acc: 0.953 - ETA: 1:23 - loss: 0.1324 - acc: 0.953 - ETA: 1:20 - loss: 0.1329 - acc: 0.953 - ETA: 1:16 - loss: 0.1330 - acc: 0.952 - ETA: 1:13 - loss: 0.1334 - acc: 0.952 - ETA: 1:10 - loss: 0.1337 - acc: 0.952 - ETA: 1:07 - loss: 0.1342 - acc: 0.952 - ETA: 1:04 - loss: 0.1339 - acc: 0.952 - ETA: 1:01 - loss: 0.1341 - acc: 0.952 - ETA: 58s - loss: 0.1338 - acc: 0.952 - ETA: 55s - loss: 0.1337 - acc: 0.95 - ETA: 52s - loss: 0.1335 - acc: 0.95 - ETA: 49s - loss: 0.1330 - acc: 0.95 - ETA: 46s - loss: 0.1331 - acc: 0.95 - ETA: 42s - loss: 0.1339 - acc: 0.95 - ETA: 40s - loss: 0.1338 - acc: 0.95 - ETA: 36s - loss: 0.1339 - acc: 0.95 - ETA: 33s - loss: 0.1336 - acc: 0.95 - ETA: 30s - loss: 0.1332 - acc: 0.95 - ETA: 27s - loss: 0.1334 - acc: 0.95 - ETA: 24s - loss: 0.1336 - acc: 0.95 - ETA: 21s - loss: 0.1335 - acc: 0.95 - ETA: 18s - loss: 0.1338 - acc: 0.95 - ETA: 15s - loss: 0.1333 - acc: 0.95 - ETA: 12s - loss: 0.1333 - acc: 0.95 - ETA: 9s - loss: 0.1336 - acc: 0.9519 - ETA: 6s - loss: 0.1340 - acc: 0.951 - ETA: 3s - loss: 0.1343 - acc: 0.951 - 240s 7ms/step - loss: 0.1345 - acc: 0.9514 - val_loss: 0.2801 - val_acc: 0.8945\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 3:05 - loss: 0.1063 - acc: 0.970 - ETA: 3:03 - loss: 0.1315 - acc: 0.958 - ETA: 3:01 - loss: 0.1324 - acc: 0.955 - ETA: 2:58 - loss: 0.1275 - acc: 0.958 - ETA: 2:55 - loss: 0.1231 - acc: 0.958 - ETA: 3:04 - loss: 0.1230 - acc: 0.958 - ETA: 2:59 - loss: 0.1192 - acc: 0.958 - ETA: 2:55 - loss: 0.1204 - acc: 0.957 - ETA: 2:52 - loss: 0.1181 - acc: 0.957 - ETA: 2:48 - loss: 0.1201 - acc: 0.956 - ETA: 2:50 - loss: 0.1217 - acc: 0.956 - ETA: 2:46 - loss: 0.1205 - acc: 0.956 - ETA: 2:42 - loss: 0.1198 - acc: 0.956 - ETA: 2:39 - loss: 0.1235 - acc: 0.955 - ETA: 2:35 - loss: 0.1271 - acc: 0.955 - ETA: 2:32 - loss: 0.1260 - acc: 0.956 - ETA: 2:32 - loss: 0.1263 - acc: 0.955 - ETA: 2:29 - loss: 0.1262 - acc: 0.955 - ETA: 2:26 - loss: 0.1254 - acc: 0.956 - ETA: 2:22 - loss: 0.1262 - acc: 0.955 - ETA: 2:19 - loss: 0.1262 - acc: 0.955 - ETA: 2:18 - loss: 0.1246 - acc: 0.956 - ETA: 2:15 - loss: 0.1239 - acc: 0.956 - ETA: 2:12 - loss: 0.1242 - acc: 0.956 - ETA: 2:09 - loss: 0.1236 - acc: 0.955 - ETA: 2:05 - loss: 0.1241 - acc: 0.956 - ETA: 2:02 - loss: 0.1243 - acc: 0.956 - ETA: 2:01 - loss: 0.1232 - acc: 0.956 - ETA: 1:58 - loss: 0.1242 - acc: 0.956 - ETA: 1:55 - loss: 0.1238 - acc: 0.956 - ETA: 1:52 - loss: 0.1243 - acc: 0.956 - ETA: 1:49 - loss: 0.1257 - acc: 0.956 - ETA: 1:47 - loss: 0.1253 - acc: 0.956 - ETA: 1:44 - loss: 0.1252 - acc: 0.956 - ETA: 1:40 - loss: 0.1268 - acc: 0.955 - ETA: 1:37 - loss: 0.1261 - acc: 0.955 - ETA: 1:35 - loss: 0.1262 - acc: 0.955 - ETA: 1:33 - loss: 0.1265 - acc: 0.955 - ETA: 1:30 - loss: 0.1276 - acc: 0.954 - ETA: 1:27 - loss: 0.1280 - acc: 0.954 - ETA: 1:24 - loss: 0.1281 - acc: 0.954 - ETA: 1:21 - loss: 0.1277 - acc: 0.954 - ETA: 1:18 - loss: 0.1276 - acc: 0.954 - ETA: 1:16 - loss: 0.1279 - acc: 0.954 - ETA: 1:13 - loss: 0.1287 - acc: 0.954 - ETA: 1:10 - loss: 0.1282 - acc: 0.954 - ETA: 1:07 - loss: 0.1283 - acc: 0.954 - ETA: 1:04 - loss: 0.1283 - acc: 0.954 - ETA: 1:01 - loss: 0.1281 - acc: 0.954 - ETA: 58s - loss: 0.1281 - acc: 0.954 - ETA: 55s - loss: 0.1289 - acc: 0.95 - ETA: 52s - loss: 0.1288 - acc: 0.95 - ETA: 49s - loss: 0.1285 - acc: 0.95 - ETA: 46s - loss: 0.1278 - acc: 0.95 - ETA: 43s - loss: 0.1273 - acc: 0.95 - ETA: 41s - loss: 0.1266 - acc: 0.95 - ETA: 38s - loss: 0.1267 - acc: 0.95 - ETA: 35s - loss: 0.1269 - acc: 0.95 - ETA: 32s - loss: 0.1266 - acc: 0.95 - ETA: 29s - loss: 0.1263 - acc: 0.95 - ETA: 26s - loss: 0.1269 - acc: 0.95 - ETA: 23s - loss: 0.1271 - acc: 0.95 - ETA: 20s - loss: 0.1272 - acc: 0.95 - ETA: 17s - loss: 0.1269 - acc: 0.95 - ETA: 14s - loss: 0.1264 - acc: 0.95 - ETA: 11s - loss: 0.1265 - acc: 0.95 - ETA: 8s - loss: 0.1265 - acc: 0.9547 - ETA: 5s - loss: 0.1266 - acc: 0.954 - ETA: 2s - loss: 0.1266 - acc: 0.954 - 233s 7ms/step - loss: 0.1271 - acc: 0.9544 - val_loss: 0.2887 - val_acc: 0.8869\n",
      "Epoch 10/12\n",
      "35000/35000 [==============================] - ETA: 3:18 - loss: 0.1138 - acc: 0.960 - ETA: 4:06 - loss: 0.1012 - acc: 0.966 - ETA: 3:42 - loss: 0.1058 - acc: 0.965 - ETA: 3:28 - loss: 0.1090 - acc: 0.963 - ETA: 3:19 - loss: 0.1132 - acc: 0.960 - ETA: 3:12 - loss: 0.1098 - acc: 0.961 - ETA: 3:07 - loss: 0.1174 - acc: 0.959 - ETA: 3:11 - loss: 0.1135 - acc: 0.960 - ETA: 3:06 - loss: 0.1136 - acc: 0.961 - ETA: 3:01 - loss: 0.1137 - acc: 0.961 - ETA: 2:56 - loss: 0.1183 - acc: 0.959 - ETA: 2:52 - loss: 0.1153 - acc: 0.960 - ETA: 2:51 - loss: 0.1150 - acc: 0.960 - ETA: 2:47 - loss: 0.1167 - acc: 0.959 - ETA: 2:43 - loss: 0.1148 - acc: 0.960 - ETA: 2:39 - loss: 0.1157 - acc: 0.959 - ETA: 2:36 - loss: 0.1167 - acc: 0.959 - ETA: 2:32 - loss: 0.1166 - acc: 0.959 - ETA: 2:31 - loss: 0.1168 - acc: 0.959 - ETA: 2:28 - loss: 0.1181 - acc: 0.959 - ETA: 2:24 - loss: 0.1166 - acc: 0.960 - ETA: 2:20 - loss: 0.1169 - acc: 0.959 - ETA: 2:17 - loss: 0.1170 - acc: 0.959 - ETA: 2:16 - loss: 0.1176 - acc: 0.959 - ETA: 2:12 - loss: 0.1179 - acc: 0.959 - ETA: 2:09 - loss: 0.1185 - acc: 0.958 - ETA: 2:06 - loss: 0.1186 - acc: 0.958 - ETA: 2:02 - loss: 0.1186 - acc: 0.958 - ETA: 1:59 - loss: 0.1181 - acc: 0.958 - ETA: 1:57 - loss: 0.1181 - acc: 0.958 - ETA: 1:54 - loss: 0.1184 - acc: 0.958 - ETA: 1:51 - loss: 0.1177 - acc: 0.958 - ETA: 1:48 - loss: 0.1172 - acc: 0.959 - ETA: 1:44 - loss: 0.1168 - acc: 0.959 - ETA: 1:42 - loss: 0.1171 - acc: 0.959 - ETA: 1:39 - loss: 0.1172 - acc: 0.959 - ETA: 1:36 - loss: 0.1166 - acc: 0.959 - ETA: 1:33 - loss: 0.1176 - acc: 0.959 - ETA: 1:30 - loss: 0.1174 - acc: 0.959 - ETA: 1:27 - loss: 0.1176 - acc: 0.959 - ETA: 1:24 - loss: 0.1174 - acc: 0.959 - ETA: 1:21 - loss: 0.1174 - acc: 0.959 - ETA: 1:18 - loss: 0.1185 - acc: 0.958 - ETA: 1:15 - loss: 0.1185 - acc: 0.958 - ETA: 1:12 - loss: 0.1178 - acc: 0.958 - ETA: 1:10 - loss: 0.1174 - acc: 0.959 - ETA: 1:07 - loss: 0.1174 - acc: 0.958 - ETA: 1:04 - loss: 0.1177 - acc: 0.958 - ETA: 1:01 - loss: 0.1183 - acc: 0.958 - ETA: 58s - loss: 0.1183 - acc: 0.958 - ETA: 55s - loss: 0.1180 - acc: 0.95 - ETA: 52s - loss: 0.1185 - acc: 0.95 - ETA: 49s - loss: 0.1186 - acc: 0.95 - ETA: 46s - loss: 0.1185 - acc: 0.95 - ETA: 43s - loss: 0.1185 - acc: 0.95 - ETA: 40s - loss: 0.1184 - acc: 0.95 - ETA: 37s - loss: 0.1183 - acc: 0.95 - ETA: 34s - loss: 0.1186 - acc: 0.95 - ETA: 32s - loss: 0.1189 - acc: 0.95 - ETA: 29s - loss: 0.1188 - acc: 0.95 - ETA: 26s - loss: 0.1188 - acc: 0.95 - ETA: 23s - loss: 0.1192 - acc: 0.95 - ETA: 20s - loss: 0.1190 - acc: 0.95 - ETA: 17s - loss: 0.1196 - acc: 0.95 - ETA: 14s - loss: 0.1194 - acc: 0.95 - ETA: 11s - loss: 0.1197 - acc: 0.95 - ETA: 8s - loss: 0.1193 - acc: 0.9573 - ETA: 5s - loss: 0.1191 - acc: 0.957 - ETA: 2s - loss: 0.1192 - acc: 0.957 - 229s 7ms/step - loss: 0.1194 - acc: 0.9572 - val_loss: 0.3001 - val_acc: 0.8934\n",
      "Epoch 11/12\n",
      "35000/35000 [==============================] - ETA: 4:10 - loss: 0.1047 - acc: 0.964 - ETA: 3:42 - loss: 0.1038 - acc: 0.964 - ETA: 3:26 - loss: 0.0959 - acc: 0.967 - ETA: 3:17 - loss: 0.0945 - acc: 0.968 - ETA: 3:10 - loss: 0.0965 - acc: 0.968 - ETA: 3:17 - loss: 0.0942 - acc: 0.970 - ETA: 3:18 - loss: 0.0942 - acc: 0.968 - ETA: 3:12 - loss: 0.0975 - acc: 0.968 - ETA: 3:06 - loss: 0.0986 - acc: 0.967 - ETA: 3:01 - loss: 0.0978 - acc: 0.967 - ETA: 3:03 - loss: 0.0996 - acc: 0.967 - ETA: 2:58 - loss: 0.1003 - acc: 0.966 - ETA: 2:54 - loss: 0.1001 - acc: 0.967 - ETA: 2:49 - loss: 0.1020 - acc: 0.966 - ETA: 2:45 - loss: 0.1015 - acc: 0.966 - ETA: 2:41 - loss: 0.1005 - acc: 0.967 - ETA: 2:41 - loss: 0.1010 - acc: 0.966 - ETA: 2:37 - loss: 0.1009 - acc: 0.966 - ETA: 2:33 - loss: 0.1002 - acc: 0.966 - ETA: 2:29 - loss: 0.1011 - acc: 0.966 - ETA: 2:25 - loss: 0.1002 - acc: 0.966 - ETA: 2:24 - loss: 0.1003 - acc: 0.965 - ETA: 2:20 - loss: 0.1006 - acc: 0.965 - ETA: 2:18 - loss: 0.1007 - acc: 0.965 - ETA: 2:16 - loss: 0.1005 - acc: 0.965 - ETA: 2:12 - loss: 0.1000 - acc: 0.965 - ETA: 2:10 - loss: 0.1011 - acc: 0.965 - ETA: 2:07 - loss: 0.1008 - acc: 0.965 - ETA: 2:03 - loss: 0.1006 - acc: 0.965 - ETA: 2:00 - loss: 0.1009 - acc: 0.965 - ETA: 1:56 - loss: 0.1007 - acc: 0.965 - ETA: 1:53 - loss: 0.1022 - acc: 0.964 - ETA: 1:51 - loss: 0.1027 - acc: 0.964 - ETA: 1:47 - loss: 0.1025 - acc: 0.964 - ETA: 1:44 - loss: 0.1026 - acc: 0.964 - ETA: 1:41 - loss: 0.1021 - acc: 0.964 - ETA: 1:38 - loss: 0.1030 - acc: 0.964 - ETA: 1:35 - loss: 0.1034 - acc: 0.963 - ETA: 1:32 - loss: 0.1038 - acc: 0.963 - ETA: 1:29 - loss: 0.1036 - acc: 0.963 - ETA: 1:26 - loss: 0.1034 - acc: 0.963 - ETA: 1:23 - loss: 0.1041 - acc: 0.963 - ETA: 1:19 - loss: 0.1056 - acc: 0.962 - ETA: 1:17 - loss: 0.1059 - acc: 0.962 - ETA: 1:14 - loss: 0.1061 - acc: 0.962 - ETA: 1:11 - loss: 0.1065 - acc: 0.962 - ETA: 1:08 - loss: 0.1067 - acc: 0.962 - ETA: 1:05 - loss: 0.1069 - acc: 0.962 - ETA: 1:02 - loss: 0.1071 - acc: 0.962 - ETA: 59s - loss: 0.1072 - acc: 0.962 - ETA: 56s - loss: 0.1076 - acc: 0.96 - ETA: 53s - loss: 0.1077 - acc: 0.96 - ETA: 50s - loss: 0.1083 - acc: 0.96 - ETA: 47s - loss: 0.1085 - acc: 0.96 - ETA: 44s - loss: 0.1086 - acc: 0.96 - ETA: 41s - loss: 0.1089 - acc: 0.96 - ETA: 38s - loss: 0.1092 - acc: 0.96 - ETA: 35s - loss: 0.1099 - acc: 0.96 - ETA: 32s - loss: 0.1100 - acc: 0.96 - ETA: 29s - loss: 0.1100 - acc: 0.96 - ETA: 26s - loss: 0.1101 - acc: 0.96 - ETA: 23s - loss: 0.1104 - acc: 0.96 - ETA: 20s - loss: 0.1103 - acc: 0.96 - ETA: 17s - loss: 0.1104 - acc: 0.96 - ETA: 14s - loss: 0.1105 - acc: 0.96 - ETA: 11s - loss: 0.1106 - acc: 0.96 - ETA: 8s - loss: 0.1103 - acc: 0.9611 - ETA: 5s - loss: 0.1107 - acc: 0.961 - ETA: 2s - loss: 0.1102 - acc: 0.961 - 232s 7ms/step - loss: 0.1103 - acc: 0.9610 - val_loss: 0.3381 - val_acc: 0.8919\n",
      "Epoch 12/12\n",
      "35000/35000 [==============================] - ETA: 3:20 - loss: 0.1320 - acc: 0.948 - ETA: 3:10 - loss: 0.1066 - acc: 0.959 - ETA: 3:05 - loss: 0.1030 - acc: 0.964 - ETA: 3:18 - loss: 0.0982 - acc: 0.965 - ETA: 3:14 - loss: 0.1083 - acc: 0.962 - ETA: 3:08 - loss: 0.1066 - acc: 0.964 - ETA: 3:03 - loss: 0.1107 - acc: 0.962 - ETA: 2:59 - loss: 0.1063 - acc: 0.965 - ETA: 3:01 - loss: 0.1016 - acc: 0.966 - ETA: 2:57 - loss: 0.1049 - acc: 0.966 - ETA: 2:53 - loss: 0.1040 - acc: 0.966 - ETA: 2:49 - loss: 0.1034 - acc: 0.966 - ETA: 2:45 - loss: 0.1023 - acc: 0.966 - ETA: 2:41 - loss: 0.1022 - acc: 0.966 - ETA: 2:41 - loss: 0.1004 - acc: 0.967 - ETA: 2:37 - loss: 0.1001 - acc: 0.967 - ETA: 2:34 - loss: 0.0990 - acc: 0.967 - ETA: 2:30 - loss: 0.0967 - acc: 0.967 - ETA: 2:27 - loss: 0.0983 - acc: 0.967 - ETA: 2:26 - loss: 0.0985 - acc: 0.967 - ETA: 2:22 - loss: 0.0979 - acc: 0.967 - ETA: 2:19 - loss: 0.0972 - acc: 0.968 - ETA: 2:16 - loss: 0.0973 - acc: 0.967 - ETA: 2:12 - loss: 0.0972 - acc: 0.967 - ETA: 2:09 - loss: 0.0971 - acc: 0.967 - ETA: 2:08 - loss: 0.0960 - acc: 0.968 - ETA: 2:05 - loss: 0.0968 - acc: 0.967 - ETA: 2:01 - loss: 0.0971 - acc: 0.967 - ETA: 1:58 - loss: 0.0987 - acc: 0.967 - ETA: 1:55 - loss: 0.0984 - acc: 0.967 - ETA: 1:53 - loss: 0.0986 - acc: 0.967 - ETA: 1:50 - loss: 0.1007 - acc: 0.966 - ETA: 1:47 - loss: 0.1004 - acc: 0.966 - ETA: 1:44 - loss: 0.1005 - acc: 0.967 - ETA: 1:41 - loss: 0.1012 - acc: 0.966 - ETA: 1:38 - loss: 0.1004 - acc: 0.966 - ETA: 1:36 - loss: 0.1009 - acc: 0.966 - ETA: 1:32 - loss: 0.1001 - acc: 0.966 - ETA: 1:29 - loss: 0.1005 - acc: 0.966 - ETA: 1:26 - loss: 0.1001 - acc: 0.966 - ETA: 1:23 - loss: 0.1010 - acc: 0.966 - ETA: 1:21 - loss: 0.1012 - acc: 0.966 - ETA: 1:18 - loss: 0.1017 - acc: 0.966 - ETA: 1:15 - loss: 0.1014 - acc: 0.966 - ETA: 1:12 - loss: 0.1009 - acc: 0.966 - ETA: 1:09 - loss: 0.1005 - acc: 0.966 - ETA: 1:06 - loss: 0.1014 - acc: 0.965 - ETA: 1:04 - loss: 0.1016 - acc: 0.965 - ETA: 1:01 - loss: 0.1025 - acc: 0.965 - ETA: 58s - loss: 0.1028 - acc: 0.965 - ETA: 55s - loss: 0.1025 - acc: 0.96 - ETA: 52s - loss: 0.1029 - acc: 0.96 - ETA: 49s - loss: 0.1029 - acc: 0.96 - ETA: 46s - loss: 0.1029 - acc: 0.96 - ETA: 43s - loss: 0.1028 - acc: 0.96 - ETA: 40s - loss: 0.1033 - acc: 0.96 - ETA: 37s - loss: 0.1029 - acc: 0.96 - ETA: 35s - loss: 0.1026 - acc: 0.96 - ETA: 32s - loss: 0.1022 - acc: 0.96 - ETA: 29s - loss: 0.1024 - acc: 0.96 - ETA: 26s - loss: 0.1024 - acc: 0.96 - ETA: 23s - loss: 0.1026 - acc: 0.96 - ETA: 20s - loss: 0.1026 - acc: 0.96 - ETA: 17s - loss: 0.1030 - acc: 0.96 - ETA: 14s - loss: 0.1029 - acc: 0.96 - ETA: 11s - loss: 0.1026 - acc: 0.96 - ETA: 8s - loss: 0.1025 - acc: 0.9650 - ETA: 5s - loss: 0.1026 - acc: 0.965 - ETA: 2s - loss: 0.1027 - acc: 0.964 - 230s 7ms/step - loss: 0.1029 - acc: 0.9649 - val_loss: 0.3256 - val_acc: 0.8846\n"
     ]
    }
   ],
   "source": [
    "model_1 = model.fit(x_train, np.array(y_train), batch_size = batch_size, epochs = epochs, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot train and cross validation error\n",
    "plot_train_cv_loss(model_1, epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 : 2 Layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 32)          1283648   \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, None, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, None, 100)         53200     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 100)         0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 80)                57920     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 81        \n",
      "=================================================================\n",
      "Total params: 1,394,977\n",
      "Trainable params: 1,394,913\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "#https://stackoverflow.com/questions/40331510/how-to-stack-multiple-lstm-in-keras\n",
    "model_2 = Sequential()\n",
    "\n",
    "#Embedding layer\n",
    "model_2.add(Embedding(vocab_size, embedding_vector_length))\n",
    "\n",
    "model_2.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model_2.add(Dropout(0.20))\n",
    "\n",
    "#LSTM layer 1\n",
    "model_2.add(LSTM(100,bias_regularizer=reg,return_sequences=True))\n",
    "\n",
    "model_2.add(Dropout(0.20))\n",
    "# LSTM layer2\n",
    "model_2.add(LSTM(80, bias_regularizer=reg))\n",
    "\n",
    "model_2.add(Dense(1, activation='relu'))\n",
    "model_2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model_2.summary())\n",
    "#Refer: https://datascience.stackexchange.com/questions/10615/number-of-parameters-in-an-lstm-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35000 samples, validate on 15000 samples\n",
      "Epoch 1/12\n",
      "35000/35000 [==============================] - ETA: 7:34 - loss: 3.6507 - acc: 0.850 - ETA: 7:57 - loss: 3.6080 - acc: 0.852 - ETA: 7:33 - loss: 3.5811 - acc: 0.862 - ETA: 7:33 - loss: 3.5803 - acc: 0.861 - ETA: 7:18 - loss: 3.5701 - acc: 0.857 - ETA: 7:08 - loss: 3.5691 - acc: 0.854 - ETA: 7:08 - loss: 3.5668 - acc: 0.855 - ETA: 6:58 - loss: 3.5664 - acc: 0.856 - ETA: 6:56 - loss: 3.5683 - acc: 0.858 - ETA: 6:47 - loss: 3.5715 - acc: 0.858 - ETA: 6:46 - loss: 3.5765 - acc: 0.858 - ETA: 6:36 - loss: 3.5790 - acc: 0.858 - ETA: 6:28 - loss: 3.5792 - acc: 0.860 - ETA: 6:24 - loss: 3.5773 - acc: 0.862 - ETA: 6:16 - loss: 3.5760 - acc: 0.862 - ETA: 6:11 - loss: 3.5725 - acc: 0.863 - ETA: 6:04 - loss: 3.5681 - acc: 0.864 - ETA: 5:59 - loss: 3.5635 - acc: 0.864 - ETA: 5:51 - loss: 3.5580 - acc: 0.864 - ETA: 5:43 - loss: 3.5533 - acc: 0.864 - ETA: 5:37 - loss: 3.5470 - acc: 0.864 - ETA: 5:29 - loss: 3.5439 - acc: 0.864 - ETA: 5:24 - loss: 3.5470 - acc: 0.863 - ETA: 5:16 - loss: 3.5489 - acc: 0.862 - ETA: 5:10 - loss: 3.5527 - acc: 0.861 - ETA: 5:03 - loss: 3.5500 - acc: 0.862 - ETA: 4:55 - loss: 3.5493 - acc: 0.862 - ETA: 4:49 - loss: 3.5458 - acc: 0.862 - ETA: 4:42 - loss: 3.5443 - acc: 0.862 - ETA: 4:37 - loss: 3.5396 - acc: 0.861 - ETA: 4:30 - loss: 3.5351 - acc: 0.861 - ETA: 4:24 - loss: 3.5309 - acc: 0.861 - ETA: 4:16 - loss: 3.5270 - acc: 0.862 - ETA: 4:11 - loss: 3.5235 - acc: 0.862 - ETA: 4:05 - loss: 3.5191 - acc: 0.863 - ETA: 3:59 - loss: 3.5149 - acc: 0.864 - ETA: 3:52 - loss: 3.5105 - acc: 0.865 - ETA: 3:45 - loss: 3.5073 - acc: 0.865 - ETA: 3:38 - loss: 3.5029 - acc: 0.865 - ETA: 3:30 - loss: 3.4992 - acc: 0.866 - ETA: 3:24 - loss: 3.4947 - acc: 0.866 - ETA: 3:16 - loss: 3.4922 - acc: 0.866 - ETA: 3:10 - loss: 3.4903 - acc: 0.866 - ETA: 3:03 - loss: 3.4876 - acc: 0.865 - ETA: 2:56 - loss: 3.4848 - acc: 0.865 - ETA: 2:49 - loss: 3.4811 - acc: 0.866 - ETA: 2:42 - loss: 3.4772 - acc: 0.866 - ETA: 2:35 - loss: 3.4748 - acc: 0.866 - ETA: 2:28 - loss: 3.4718 - acc: 0.866 - ETA: 2:21 - loss: 3.4700 - acc: 0.866 - ETA: 2:14 - loss: 3.4659 - acc: 0.866 - ETA: 2:07 - loss: 3.4634 - acc: 0.867 - ETA: 2:00 - loss: 3.4593 - acc: 0.867 - ETA: 1:53 - loss: 3.4553 - acc: 0.868 - ETA: 1:46 - loss: 3.4528 - acc: 0.868 - ETA: 1:39 - loss: 3.4502 - acc: 0.868 - ETA: 1:31 - loss: 3.4467 - acc: 0.869 - ETA: 1:24 - loss: 3.4441 - acc: 0.869 - ETA: 1:17 - loss: 3.4416 - acc: 0.869 - ETA: 1:10 - loss: 3.4390 - acc: 0.869 - ETA: 1:03 - loss: 3.4363 - acc: 0.869 - ETA: 56s - loss: 3.4328 - acc: 0.869 - ETA: 49s - loss: 3.4290 - acc: 0.87 - ETA: 42s - loss: 3.4253 - acc: 0.87 - ETA: 35s - loss: 3.4216 - acc: 0.87 - ETA: 28s - loss: 3.4195 - acc: 0.87 - ETA: 21s - loss: 3.4185 - acc: 0.87 - ETA: 14s - loss: 3.4168 - acc: 0.87 - ETA: 7s - loss: 3.4136 - acc: 0.8718 - 558s 16ms/step - loss: 3.4116 - acc: 0.8719 - val_loss: 3.4214 - val_acc: 0.8612\n",
      "Epoch 2/12\n",
      "35000/35000 [==============================] - ETA: 8:09 - loss: 3.2134 - acc: 0.888 - ETA: 8:19 - loss: 3.1863 - acc: 0.886 - ETA: 7:54 - loss: 3.1661 - acc: 0.887 - ETA: 7:46 - loss: 3.1700 - acc: 0.888 - ETA: 7:39 - loss: 3.1641 - acc: 0.893 - ETA: 7:27 - loss: 3.1708 - acc: 0.893 - ETA: 7:26 - loss: 3.1673 - acc: 0.890 - ETA: 7:17 - loss: 3.1583 - acc: 0.891 - ETA: 7:13 - loss: 3.1617 - acc: 0.891 - ETA: 7:04 - loss: 3.1602 - acc: 0.891 - ETA: 7:00 - loss: 3.1583 - acc: 0.891 - ETA: 6:49 - loss: 3.1519 - acc: 0.891 - ETA: 6:40 - loss: 3.1493 - acc: 0.892 - ETA: 6:36 - loss: 3.1525 - acc: 0.892 - ETA: 6:28 - loss: 3.1549 - acc: 0.891 - ETA: 6:24 - loss: 3.1546 - acc: 0.890 - ETA: 6:15 - loss: 3.1524 - acc: 0.890 - ETA: 6:12 - loss: 3.1525 - acc: 0.890 - ETA: 6:04 - loss: 3.1502 - acc: 0.890 - ETA: 6:03 - loss: 3.1487 - acc: 0.889 - ETA: 5:56 - loss: 3.1479 - acc: 0.889 - ETA: 5:50 - loss: 3.1466 - acc: 0.890 - ETA: 5:43 - loss: 3.1456 - acc: 0.890 - ETA: 5:36 - loss: 3.1447 - acc: 0.889 - ETA: 5:27 - loss: 3.1435 - acc: 0.889 - ETA: 5:21 - loss: 3.1407 - acc: 0.890 - ETA: 5:12 - loss: 3.1379 - acc: 0.890 - ETA: 5:04 - loss: 3.1359 - acc: 0.891 - ETA: 4:58 - loss: 3.1333 - acc: 0.891 - ETA: 4:50 - loss: 3.1345 - acc: 0.890 - ETA: 4:43 - loss: 3.1320 - acc: 0.890 - ETA: 4:35 - loss: 3.1315 - acc: 0.890 - ETA: 4:28 - loss: 3.1291 - acc: 0.890 - ETA: 4:20 - loss: 3.1267 - acc: 0.890 - ETA: 4:14 - loss: 3.1268 - acc: 0.889 - ETA: 4:07 - loss: 3.1272 - acc: 0.889 - ETA: 4:00 - loss: 3.1264 - acc: 0.890 - ETA: 3:52 - loss: 3.1265 - acc: 0.890 - ETA: 3:44 - loss: 3.1240 - acc: 0.890 - ETA: 3:38 - loss: 3.1223 - acc: 0.890 - ETA: 3:30 - loss: 3.1215 - acc: 0.889 - ETA: 3:23 - loss: 3.1186 - acc: 0.889 - ETA: 3:16 - loss: 3.1158 - acc: 0.890 - ETA: 3:09 - loss: 3.1130 - acc: 0.890 - ETA: 3:01 - loss: 3.1111 - acc: 0.889 - ETA: 2:54 - loss: 3.1093 - acc: 0.890 - ETA: 2:47 - loss: 3.1066 - acc: 0.890 - ETA: 2:40 - loss: 3.1032 - acc: 0.890 - ETA: 2:32 - loss: 3.0998 - acc: 0.890 - ETA: 2:25 - loss: 3.0983 - acc: 0.890 - ETA: 2:18 - loss: 3.0978 - acc: 0.890 - ETA: 2:10 - loss: 3.0967 - acc: 0.890 - ETA: 2:03 - loss: 3.0937 - acc: 0.890 - ETA: 1:55 - loss: 3.0927 - acc: 0.890 - ETA: 1:48 - loss: 3.0894 - acc: 0.890 - ETA: 1:41 - loss: 3.0876 - acc: 0.890 - ETA: 1:34 - loss: 3.0846 - acc: 0.890 - ETA: 1:26 - loss: 3.0818 - acc: 0.890 - ETA: 1:19 - loss: 3.0799 - acc: 0.890 - ETA: 1:12 - loss: 3.0767 - acc: 0.891 - ETA: 1:05 - loss: 3.0746 - acc: 0.891 - ETA: 58s - loss: 3.0719 - acc: 0.891 - ETA: 50s - loss: 3.0690 - acc: 0.89 - ETA: 43s - loss: 3.0674 - acc: 0.89 - ETA: 36s - loss: 3.0652 - acc: 0.89 - ETA: 29s - loss: 3.0629 - acc: 0.89 - ETA: 21s - loss: 3.0612 - acc: 0.89 - ETA: 14s - loss: 3.0589 - acc: 0.89 - ETA: 7s - loss: 3.0561 - acc: 0.8914 - 571s 16ms/step - loss: 3.0529 - acc: 0.8917 - val_loss: 3.1616 - val_acc: 0.8686\n",
      "Epoch 3/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 8:45 - loss: 2.8238 - acc: 0.918 - ETA: 8:06 - loss: 2.8384 - acc: 0.899 - ETA: 8:13 - loss: 2.8542 - acc: 0.900 - ETA: 7:55 - loss: 2.8524 - acc: 0.896 - ETA: 7:56 - loss: 2.8493 - acc: 0.898 - ETA: 7:42 - loss: 2.8454 - acc: 0.899 - ETA: 7:42 - loss: 2.8604 - acc: 0.896 - ETA: 7:28 - loss: 2.8535 - acc: 0.897 - ETA: 7:18 - loss: 2.8513 - acc: 0.899 - ETA: 7:12 - loss: 2.8458 - acc: 0.899 - ETA: 7:03 - loss: 2.8438 - acc: 0.899 - ETA: 7:03 - loss: 2.8413 - acc: 0.901 - ETA: 6:54 - loss: 2.8372 - acc: 0.900 - ETA: 6:49 - loss: 2.8302 - acc: 0.902 - ETA: 6:40 - loss: 2.8273 - acc: 0.902 - ETA: 6:34 - loss: 2.8240 - acc: 0.903 - ETA: 6:25 - loss: 2.8223 - acc: 0.903 - ETA: 6:20 - loss: 2.8185 - acc: 0.904 - ETA: 6:12 - loss: 2.8168 - acc: 0.904 - ETA: 6:05 - loss: 2.8181 - acc: 0.904 - ETA: 5:56 - loss: 2.8153 - acc: 0.904 - ETA: 5:48 - loss: 2.8153 - acc: 0.905 - ETA: 5:42 - loss: 2.8156 - acc: 0.904 - ETA: 5:34 - loss: 2.8142 - acc: 0.904 - ETA: 5:27 - loss: 2.8104 - acc: 0.905 - ETA: 5:19 - loss: 2.8081 - acc: 0.905 - ETA: 5:14 - loss: 2.8058 - acc: 0.905 - ETA: 5:06 - loss: 2.8025 - acc: 0.906 - ETA: 5:00 - loss: 2.8024 - acc: 0.906 - ETA: 4:52 - loss: 2.8004 - acc: 0.906 - ETA: 4:46 - loss: 2.7990 - acc: 0.905 - ETA: 4:38 - loss: 2.7995 - acc: 0.906 - ETA: 4:32 - loss: 2.7970 - acc: 0.906 - ETA: 4:24 - loss: 2.7930 - acc: 0.907 - ETA: 4:16 - loss: 2.7896 - acc: 0.906 - ETA: 4:09 - loss: 2.7904 - acc: 0.906 - ETA: 4:01 - loss: 2.7882 - acc: 0.906 - ETA: 3:54 - loss: 2.7857 - acc: 0.905 - ETA: 3:48 - loss: 2.7832 - acc: 0.905 - ETA: 3:41 - loss: 2.7822 - acc: 0.906 - ETA: 3:33 - loss: 2.7809 - acc: 0.906 - ETA: 3:26 - loss: 2.7772 - acc: 0.907 - ETA: 3:18 - loss: 2.7770 - acc: 0.906 - ETA: 3:11 - loss: 2.7734 - acc: 0.907 - ETA: 3:03 - loss: 2.7724 - acc: 0.907 - ETA: 2:57 - loss: 2.7712 - acc: 0.907 - ETA: 2:49 - loss: 2.7688 - acc: 0.908 - ETA: 2:42 - loss: 2.7661 - acc: 0.908 - ETA: 2:34 - loss: 2.7640 - acc: 0.908 - ETA: 2:27 - loss: 2.7636 - acc: 0.907 - ETA: 2:19 - loss: 2.7621 - acc: 0.907 - ETA: 2:12 - loss: 2.7609 - acc: 0.906 - ETA: 2:05 - loss: 2.7590 - acc: 0.906 - ETA: 1:57 - loss: 2.7577 - acc: 0.906 - ETA: 1:50 - loss: 2.7551 - acc: 0.906 - ETA: 1:43 - loss: 2.7527 - acc: 0.907 - ETA: 1:35 - loss: 2.7508 - acc: 0.906 - ETA: 1:28 - loss: 2.7491 - acc: 0.906 - ETA: 1:21 - loss: 2.7485 - acc: 0.906 - ETA: 1:13 - loss: 2.7466 - acc: 0.906 - ETA: 1:06 - loss: 2.7456 - acc: 0.906 - ETA: 58s - loss: 2.7485 - acc: 0.905 - ETA: 51s - loss: 2.7484 - acc: 0.90 - ETA: 44s - loss: 2.7493 - acc: 0.90 - ETA: 36s - loss: 2.7496 - acc: 0.90 - ETA: 29s - loss: 2.7489 - acc: 0.90 - ETA: 22s - loss: 2.7473 - acc: 0.90 - ETA: 14s - loss: 2.7447 - acc: 0.90 - ETA: 7s - loss: 2.7426 - acc: 0.9058 - 575s 16ms/step - loss: 2.7417 - acc: 0.9056 - val_loss: 2.9067 - val_acc: 0.8820\n",
      "Epoch 4/12\n",
      "35000/35000 [==============================] - ETA: 12:16 - loss: 2.6126 - acc: 0.90 - ETA: 9:58 - loss: 2.5676 - acc: 0.9080 - ETA: 9:23 - loss: 2.5568 - acc: 0.914 - ETA: 8:51 - loss: 2.5461 - acc: 0.917 - ETA: 8:52 - loss: 2.5540 - acc: 0.916 - ETA: 8:38 - loss: 2.5548 - acc: 0.919 - ETA: 8:39 - loss: 2.5523 - acc: 0.919 - ETA: 8:21 - loss: 2.5527 - acc: 0.920 - ETA: 8:12 - loss: 2.5573 - acc: 0.919 - ETA: 7:55 - loss: 2.5524 - acc: 0.919 - ETA: 7:47 - loss: 2.5448 - acc: 0.920 - ETA: 7:34 - loss: 2.5366 - acc: 0.922 - ETA: 7:28 - loss: 2.5331 - acc: 0.922 - ETA: 7:15 - loss: 2.5310 - acc: 0.922 - ETA: 7:08 - loss: 2.5311 - acc: 0.921 - ETA: 6:57 - loss: 2.5304 - acc: 0.919 - ETA: 6:50 - loss: 2.5311 - acc: 0.920 - ETA: 6:39 - loss: 2.5347 - acc: 0.919 - ETA: 6:30 - loss: 2.5337 - acc: 0.919 - ETA: 6:23 - loss: 2.5326 - acc: 0.919 - ETA: 6:20 - loss: 2.5280 - acc: 0.919 - ETA: 6:10 - loss: 2.5238 - acc: 0.919 - ETA: 6:00 - loss: 2.5220 - acc: 0.919 - ETA: 5:54 - loss: 2.5212 - acc: 0.919 - ETA: 5:45 - loss: 2.5224 - acc: 0.918 - ETA: 5:38 - loss: 2.5254 - acc: 0.917 - ETA: 5:29 - loss: 2.5231 - acc: 0.918 - ETA: 5:22 - loss: 2.5256 - acc: 0.916 - ETA: 5:13 - loss: 2.5277 - acc: 0.916 - ETA: 5:06 - loss: 2.5273 - acc: 0.915 - ETA: 4:58 - loss: 2.5275 - acc: 0.914 - ETA: 4:50 - loss: 2.5259 - acc: 0.914 - ETA: 4:42 - loss: 2.5241 - acc: 0.914 - ETA: 4:35 - loss: 2.5225 - acc: 0.914 - ETA: 4:27 - loss: 2.5219 - acc: 0.913 - ETA: 4:19 - loss: 2.5202 - acc: 0.913 - ETA: 4:11 - loss: 2.5218 - acc: 0.913 - ETA: 4:03 - loss: 2.5197 - acc: 0.913 - ETA: 3:56 - loss: 2.5191 - acc: 0.913 - ETA: 3:47 - loss: 2.5169 - acc: 0.912 - ETA: 3:41 - loss: 2.5165 - acc: 0.912 - ETA: 3:33 - loss: 2.5150 - acc: 0.911 - ETA: 3:26 - loss: 2.5130 - acc: 0.912 - ETA: 3:18 - loss: 2.5119 - acc: 0.912 - ETA: 3:10 - loss: 2.5115 - acc: 0.912 - ETA: 3:02 - loss: 2.5122 - acc: 0.912 - ETA: 2:55 - loss: 2.5135 - acc: 0.910 - ETA: 2:47 - loss: 2.5142 - acc: 0.910 - ETA: 2:40 - loss: 2.5155 - acc: 0.909 - ETA: 2:32 - loss: 2.5152 - acc: 0.909 - ETA: 2:24 - loss: 2.5143 - acc: 0.909 - ETA: 2:17 - loss: 2.5131 - acc: 0.909 - ETA: 2:09 - loss: 2.5110 - acc: 0.910 - ETA: 2:01 - loss: 2.5090 - acc: 0.910 - ETA: 1:53 - loss: 2.5066 - acc: 0.910 - ETA: 1:46 - loss: 2.5036 - acc: 0.909 - ETA: 1:38 - loss: 2.5019 - acc: 0.909 - ETA: 1:31 - loss: 2.5015 - acc: 0.909 - ETA: 1:23 - loss: 2.5031 - acc: 0.908 - ETA: 1:15 - loss: 2.5055 - acc: 0.907 - ETA: 1:08 - loss: 2.5075 - acc: 0.907 - ETA: 1:00 - loss: 2.5058 - acc: 0.906 - ETA: 53s - loss: 2.5042 - acc: 0.906 - ETA: 45s - loss: 2.5025 - acc: 0.90 - ETA: 38s - loss: 2.5016 - acc: 0.90 - ETA: 30s - loss: 2.5005 - acc: 0.90 - ETA: 22s - loss: 2.5008 - acc: 0.90 - ETA: 15s - loss: 2.5007 - acc: 0.90 - ETA: 7s - loss: 2.4982 - acc: 0.9034 - 592s 17ms/step - loss: 2.4956 - acc: 0.9034 - val_loss: 2.6620 - val_acc: 0.8753\n",
      "Epoch 5/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 9:18 - loss: 2.2621 - acc: 0.932 - ETA: 9:19 - loss: 2.2699 - acc: 0.920 - ETA: 9:16 - loss: 2.2560 - acc: 0.922 - ETA: 8:46 - loss: 2.2676 - acc: 0.915 - ETA: 8:44 - loss: 2.2646 - acc: 0.913 - ETA: 8:34 - loss: 2.2816 - acc: 0.915 - ETA: 8:28 - loss: 2.3108 - acc: 0.911 - ETA: 8:12 - loss: 2.3101 - acc: 0.914 - ETA: 8:06 - loss: 2.3010 - acc: 0.915 - ETA: 7:53 - loss: 2.3056 - acc: 0.915 - ETA: 7:47 - loss: 2.3019 - acc: 0.916 - ETA: 7:35 - loss: 2.3063 - acc: 0.914 - ETA: 7:35 - loss: 2.3228 - acc: 0.905 - ETA: 7:28 - loss: 2.3422 - acc: 0.892 - ETA: 7:21 - loss: 2.3497 - acc: 0.891 - ETA: 7:17 - loss: 2.3561 - acc: 0.892 - ETA: 7:10 - loss: 2.3639 - acc: 0.890 - ETA: 7:03 - loss: 2.3706 - acc: 0.888 - ETA: 6:52 - loss: 2.3776 - acc: 0.885 - ETA: 6:41 - loss: 2.3853 - acc: 0.884 - ETA: 6:34 - loss: 2.3942 - acc: 0.882 - ETA: 6:24 - loss: 2.4008 - acc: 0.881 - ETA: 6:21 - loss: 2.4050 - acc: 0.880 - ETA: 6:12 - loss: 2.4079 - acc: 0.878 - ETA: 6:06 - loss: 2.4099 - acc: 0.878 - ETA: 6:02 - loss: 2.4108 - acc: 0.877 - ETA: 5:53 - loss: 2.4103 - acc: 0.877 - ETA: 5:46 - loss: 2.4087 - acc: 0.877 - ETA: 5:38 - loss: 2.4087 - acc: 0.876 - ETA: 5:29 - loss: 2.4082 - acc: 0.876 - ETA: 5:20 - loss: 2.4061 - acc: 0.875 - ETA: 5:13 - loss: 2.4040 - acc: 0.875 - ETA: 5:03 - loss: 2.4043 - acc: 0.874 - ETA: 4:55 - loss: 2.4035 - acc: 0.873 - ETA: 4:48 - loss: 2.4013 - acc: 0.872 - ETA: 4:43 - loss: 2.4006 - acc: 0.871 - ETA: 4:36 - loss: 2.4017 - acc: 0.870 - ETA: 4:28 - loss: 2.4042 - acc: 0.869 - ETA: 4:21 - loss: 2.4078 - acc: 0.865 - ETA: 4:13 - loss: 2.4117 - acc: 0.862 - ETA: 4:06 - loss: 2.4152 - acc: 0.858 - ETA: 3:58 - loss: 2.4188 - acc: 0.854 - ETA: 3:49 - loss: 2.4214 - acc: 0.851 - ETA: 3:41 - loss: 2.4232 - acc: 0.849 - ETA: 3:32 - loss: 2.4241 - acc: 0.848 - ETA: 3:23 - loss: 2.4242 - acc: 0.848 - ETA: 3:14 - loss: 2.4233 - acc: 0.848 - ETA: 3:06 - loss: 2.4220 - acc: 0.848 - ETA: 2:57 - loss: 2.4193 - acc: 0.848 - ETA: 2:48 - loss: 2.4160 - acc: 0.849 - ETA: 2:40 - loss: 2.4120 - acc: 0.850 - ETA: 2:32 - loss: 2.4085 - acc: 0.850 - ETA: 2:23 - loss: 2.4042 - acc: 0.850 - ETA: 2:14 - loss: 2.4039 - acc: 0.850 - ETA: 2:05 - loss: 2.4014 - acc: 0.850 - ETA: 1:57 - loss: 2.4007 - acc: 0.850 - ETA: 1:48 - loss: 2.4008 - acc: 0.850 - ETA: 1:40 - loss: 2.3982 - acc: 0.851 - ETA: 1:31 - loss: 2.3970 - acc: 0.851 - ETA: 1:23 - loss: 2.3957 - acc: 0.851 - ETA: 1:15 - loss: 2.3921 - acc: 0.852 - ETA: 1:07 - loss: 2.3902 - acc: 0.852 - ETA: 58s - loss: 2.3882 - acc: 0.852 - ETA: 50s - loss: 2.3851 - acc: 0.85 - ETA: 41s - loss: 2.3821 - acc: 0.85 - ETA: 33s - loss: 2.3787 - acc: 0.85 - ETA: 24s - loss: 2.3758 - acc: 0.85 - ETA: 16s - loss: 2.3727 - acc: 0.85 - ETA: 8s - loss: 2.3703 - acc: 0.8541 - 646s 18ms/step - loss: 2.3676 - acc: 0.8545 - val_loss: 2.1964 - val_acc: 0.8601\n",
      "Epoch 6/12\n",
      "35000/35000 [==============================] - ETA: 8:42 - loss: 2.2242 - acc: 0.868 - ETA: 8:58 - loss: 2.2062 - acc: 0.866 - ETA: 8:51 - loss: 2.1916 - acc: 0.871 - ETA: 8:49 - loss: 2.1777 - acc: 0.875 - ETA: 8:28 - loss: 2.1709 - acc: 0.878 - ETA: 8:25 - loss: 2.1601 - acc: 0.879 - ETA: 8:11 - loss: 2.1506 - acc: 0.881 - ETA: 8:07 - loss: 2.1441 - acc: 0.882 - ETA: 7:54 - loss: 2.1370 - acc: 0.883 - ETA: 7:50 - loss: 2.1316 - acc: 0.884 - ETA: 7:38 - loss: 2.1313 - acc: 0.882 - ETA: 7:34 - loss: 2.1302 - acc: 0.882 - ETA: 7:23 - loss: 2.1278 - acc: 0.882 - ETA: 7:17 - loss: 2.1260 - acc: 0.881 - ETA: 7:07 - loss: 2.1226 - acc: 0.880 - ETA: 7:02 - loss: 2.1170 - acc: 0.881 - ETA: 6:52 - loss: 2.1139 - acc: 0.882 - ETA: 6:49 - loss: 2.1123 - acc: 0.882 - ETA: 6:40 - loss: 2.1129 - acc: 0.881 - ETA: 6:32 - loss: 2.1092 - acc: 0.882 - ETA: 6:25 - loss: 2.1068 - acc: 0.882 - ETA: 6:16 - loss: 2.1066 - acc: 0.881 - ETA: 6:07 - loss: 2.1027 - acc: 0.883 - ETA: 5:59 - loss: 2.0999 - acc: 0.883 - ETA: 5:52 - loss: 2.0968 - acc: 0.883 - ETA: 5:43 - loss: 2.0927 - acc: 0.884 - ETA: 5:36 - loss: 2.0903 - acc: 0.885 - ETA: 5:27 - loss: 2.0881 - acc: 0.884 - ETA: 5:20 - loss: 2.0862 - acc: 0.884 - ETA: 5:17 - loss: 2.0830 - acc: 0.885 - ETA: 5:09 - loss: 2.0813 - acc: 0.885 - ETA: 5:01 - loss: 2.0785 - acc: 0.885 - ETA: 4:53 - loss: 2.0772 - acc: 0.885 - ETA: 4:45 - loss: 2.0750 - acc: 0.885 - ETA: 4:37 - loss: 2.0738 - acc: 0.885 - ETA: 4:30 - loss: 2.0717 - acc: 0.885 - ETA: 4:21 - loss: 2.0703 - acc: 0.885 - ETA: 4:14 - loss: 2.0685 - acc: 0.886 - ETA: 4:05 - loss: 2.0666 - acc: 0.886 - ETA: 3:58 - loss: 2.0641 - acc: 0.886 - ETA: 3:50 - loss: 2.0626 - acc: 0.887 - ETA: 3:42 - loss: 2.0600 - acc: 0.887 - ETA: 3:34 - loss: 2.0580 - acc: 0.887 - ETA: 3:26 - loss: 2.0558 - acc: 0.888 - ETA: 3:18 - loss: 2.0541 - acc: 0.888 - ETA: 3:10 - loss: 2.0535 - acc: 0.888 - ETA: 3:02 - loss: 2.0543 - acc: 0.888 - ETA: 2:55 - loss: 2.0530 - acc: 0.888 - ETA: 2:46 - loss: 2.0531 - acc: 0.888 - ETA: 2:39 - loss: 2.0516 - acc: 0.888 - ETA: 2:30 - loss: 2.0491 - acc: 0.888 - ETA: 2:23 - loss: 2.0478 - acc: 0.888 - ETA: 2:15 - loss: 2.0459 - acc: 0.889 - ETA: 2:07 - loss: 2.0450 - acc: 0.889 - ETA: 1:59 - loss: 2.0425 - acc: 0.889 - ETA: 1:51 - loss: 2.0405 - acc: 0.889 - ETA: 1:43 - loss: 2.0380 - acc: 0.889 - ETA: 1:35 - loss: 2.0363 - acc: 0.889 - ETA: 1:27 - loss: 2.0342 - acc: 0.889 - ETA: 1:19 - loss: 2.0323 - acc: 0.889 - ETA: 1:11 - loss: 2.0298 - acc: 0.889 - ETA: 1:03 - loss: 2.0292 - acc: 0.889 - ETA: 55s - loss: 2.0273 - acc: 0.889 - ETA: 47s - loss: 2.0260 - acc: 0.88 - ETA: 39s - loss: 2.0241 - acc: 0.88 - ETA: 31s - loss: 2.0217 - acc: 0.88 - ETA: 23s - loss: 2.0202 - acc: 0.88 - ETA: 15s - loss: 2.0197 - acc: 0.89 - ETA: 7s - loss: 2.0182 - acc: 0.8899 - 618s 18ms/step - loss: 2.0161 - acc: 0.8901 - val_loss: 2.0931 - val_acc: 0.8717\n",
      "Epoch 7/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 9:46 - loss: 1.8278 - acc: 0.912 - ETA: 8:58 - loss: 1.8421 - acc: 0.915 - ETA: 8:59 - loss: 1.8294 - acc: 0.917 - ETA: 8:40 - loss: 1.8402 - acc: 0.912 - ETA: 8:41 - loss: 1.8585 - acc: 0.909 - ETA: 8:26 - loss: 1.8685 - acc: 0.909 - ETA: 8:22 - loss: 1.8570 - acc: 0.910 - ETA: 8:20 - loss: 1.8561 - acc: 0.909 - ETA: 8:14 - loss: 1.8541 - acc: 0.908 - ETA: 8:02 - loss: 1.8591 - acc: 0.907 - ETA: 7:55 - loss: 1.8605 - acc: 0.906 - ETA: 7:44 - loss: 1.8601 - acc: 0.905 - ETA: 7:38 - loss: 1.8572 - acc: 0.906 - ETA: 7:28 - loss: 1.8534 - acc: 0.905 - ETA: 7:22 - loss: 1.8519 - acc: 0.906 - ETA: 7:12 - loss: 1.8470 - acc: 0.906 - ETA: 7:09 - loss: 1.8418 - acc: 0.907 - ETA: 7:06 - loss: 1.8410 - acc: 0.907 - ETA: 6:56 - loss: 1.8388 - acc: 0.907 - ETA: 6:49 - loss: 1.8406 - acc: 0.908 - ETA: 6:39 - loss: 1.8375 - acc: 0.909 - ETA: 6:31 - loss: 1.8384 - acc: 0.908 - ETA: 6:21 - loss: 1.8354 - acc: 0.907 - ETA: 6:14 - loss: 1.8395 - acc: 0.907 - ETA: 6:04 - loss: 1.8392 - acc: 0.907 - ETA: 5:57 - loss: 1.8379 - acc: 0.907 - ETA: 5:48 - loss: 1.8373 - acc: 0.906 - ETA: 5:40 - loss: 1.8371 - acc: 0.906 - ETA: 5:31 - loss: 1.8380 - acc: 0.905 - ETA: 5:25 - loss: 1.8371 - acc: 0.905 - ETA: 5:16 - loss: 1.8374 - acc: 0.905 - ETA: 5:10 - loss: 1.8378 - acc: 0.905 - ETA: 5:01 - loss: 1.8367 - acc: 0.905 - ETA: 4:53 - loss: 1.8350 - acc: 0.905 - ETA: 4:44 - loss: 1.8316 - acc: 0.906 - ETA: 4:36 - loss: 1.8290 - acc: 0.907 - ETA: 4:28 - loss: 1.8280 - acc: 0.906 - ETA: 4:20 - loss: 1.8251 - acc: 0.906 - ETA: 4:11 - loss: 1.8242 - acc: 0.907 - ETA: 4:03 - loss: 1.8231 - acc: 0.906 - ETA: 3:55 - loss: 1.8222 - acc: 0.906 - ETA: 3:47 - loss: 1.8218 - acc: 0.906 - ETA: 3:39 - loss: 1.8207 - acc: 0.905 - ETA: 3:31 - loss: 1.8187 - acc: 0.905 - ETA: 3:23 - loss: 1.8160 - acc: 0.905 - ETA: 3:15 - loss: 1.8129 - acc: 0.906 - ETA: 3:06 - loss: 1.8112 - acc: 0.905 - ETA: 2:58 - loss: 1.8085 - acc: 0.906 - ETA: 2:50 - loss: 1.8056 - acc: 0.906 - ETA: 2:42 - loss: 1.8032 - acc: 0.906 - ETA: 2:34 - loss: 1.8019 - acc: 0.906 - ETA: 2:26 - loss: 1.8010 - acc: 0.906 - ETA: 2:18 - loss: 1.7995 - acc: 0.906 - ETA: 2:10 - loss: 1.7971 - acc: 0.906 - ETA: 2:02 - loss: 1.7945 - acc: 0.906 - ETA: 1:54 - loss: 1.7928 - acc: 0.906 - ETA: 1:45 - loss: 1.7907 - acc: 0.906 - ETA: 1:37 - loss: 1.7884 - acc: 0.907 - ETA: 1:29 - loss: 1.7865 - acc: 0.907 - ETA: 1:21 - loss: 1.7850 - acc: 0.907 - ETA: 1:13 - loss: 1.7830 - acc: 0.907 - ETA: 1:05 - loss: 1.7812 - acc: 0.907 - ETA: 57s - loss: 1.7790 - acc: 0.907 - ETA: 49s - loss: 1.7770 - acc: 0.90 - ETA: 40s - loss: 1.7754 - acc: 0.90 - ETA: 32s - loss: 1.7740 - acc: 0.90 - ETA: 24s - loss: 1.7719 - acc: 0.90 - ETA: 16s - loss: 1.7706 - acc: 0.90 - ETA: 8s - loss: 1.7708 - acc: 0.9070 - 634s 18ms/step - loss: 1.7693 - acc: 0.9071 - val_loss: 1.8895 - val_acc: 0.8795\n",
      "Epoch 8/12\n",
      "35000/35000 [==============================] - ETA: 9:01 - loss: 1.6178 - acc: 0.938 - ETA: 9:03 - loss: 1.6093 - acc: 0.921 - ETA: 9:03 - loss: 1.6052 - acc: 0.920 - ETA: 8:49 - loss: 1.6153 - acc: 0.917 - ETA: 8:36 - loss: 1.6083 - acc: 0.920 - ETA: 8:28 - loss: 1.6100 - acc: 0.919 - ETA: 8:16 - loss: 1.6050 - acc: 0.920 - ETA: 8:10 - loss: 1.6147 - acc: 0.917 - ETA: 8:00 - loss: 1.6125 - acc: 0.918 - ETA: 7:56 - loss: 1.6103 - acc: 0.916 - ETA: 7:46 - loss: 1.6097 - acc: 0.915 - ETA: 7:41 - loss: 1.6072 - acc: 0.916 - ETA: 7:31 - loss: 1.6071 - acc: 0.915 - ETA: 7:26 - loss: 1.6045 - acc: 0.916 - ETA: 7:22 - loss: 1.6002 - acc: 0.917 - ETA: 7:13 - loss: 1.5971 - acc: 0.918 - ETA: 7:06 - loss: 1.5950 - acc: 0.917 - ETA: 6:58 - loss: 1.5946 - acc: 0.918 - ETA: 6:52 - loss: 1.5935 - acc: 0.918 - ETA: 6:43 - loss: 1.5943 - acc: 0.918 - ETA: 6:39 - loss: 1.5966 - acc: 0.918 - ETA: 6:30 - loss: 1.5949 - acc: 0.918 - ETA: 6:23 - loss: 1.5988 - acc: 0.917 - ETA: 6:14 - loss: 1.5975 - acc: 0.917 - ETA: 6:08 - loss: 1.5938 - acc: 0.917 - ETA: 5:59 - loss: 1.5928 - acc: 0.917 - ETA: 5:51 - loss: 1.5917 - acc: 0.918 - ETA: 5:44 - loss: 1.5911 - acc: 0.918 - ETA: 5:36 - loss: 1.5899 - acc: 0.918 - ETA: 5:27 - loss: 1.5889 - acc: 0.919 - ETA: 5:20 - loss: 1.5874 - acc: 0.920 - ETA: 5:11 - loss: 1.5845 - acc: 0.920 - ETA: 5:03 - loss: 1.5832 - acc: 0.920 - ETA: 4:55 - loss: 1.5830 - acc: 0.920 - ETA: 4:47 - loss: 1.5798 - acc: 0.920 - ETA: 4:38 - loss: 1.5782 - acc: 0.921 - ETA: 4:30 - loss: 1.5786 - acc: 0.921 - ETA: 4:23 - loss: 1.5774 - acc: 0.921 - ETA: 4:14 - loss: 1.5770 - acc: 0.921 - ETA: 4:06 - loss: 1.5744 - acc: 0.921 - ETA: 3:58 - loss: 1.5720 - acc: 0.922 - ETA: 3:50 - loss: 1.5711 - acc: 0.922 - ETA: 3:42 - loss: 1.5712 - acc: 0.922 - ETA: 3:34 - loss: 1.5695 - acc: 0.922 - ETA: 3:25 - loss: 1.5678 - acc: 0.922 - ETA: 3:17 - loss: 1.5687 - acc: 0.922 - ETA: 3:09 - loss: 1.5679 - acc: 0.922 - ETA: 3:02 - loss: 1.5667 - acc: 0.922 - ETA: 2:53 - loss: 1.5651 - acc: 0.922 - ETA: 2:45 - loss: 1.5638 - acc: 0.922 - ETA: 2:36 - loss: 1.5625 - acc: 0.922 - ETA: 2:28 - loss: 1.5602 - acc: 0.923 - ETA: 2:20 - loss: 1.5582 - acc: 0.923 - ETA: 2:12 - loss: 1.5558 - acc: 0.923 - ETA: 2:03 - loss: 1.5559 - acc: 0.923 - ETA: 1:55 - loss: 1.5532 - acc: 0.923 - ETA: 1:47 - loss: 1.5521 - acc: 0.923 - ETA: 1:39 - loss: 1.5513 - acc: 0.923 - ETA: 1:30 - loss: 1.5500 - acc: 0.923 - ETA: 1:22 - loss: 1.5487 - acc: 0.923 - ETA: 1:14 - loss: 1.5477 - acc: 0.923 - ETA: 1:06 - loss: 1.5477 - acc: 0.923 - ETA: 57s - loss: 1.5464 - acc: 0.923 - ETA: 49s - loss: 1.5453 - acc: 0.92 - ETA: 41s - loss: 1.5444 - acc: 0.92 - ETA: 33s - loss: 1.5438 - acc: 0.92 - ETA: 25s - loss: 1.5426 - acc: 0.92 - ETA: 16s - loss: 1.5442 - acc: 0.92 - ETA: 8s - loss: 1.5483 - acc: 0.9225 - 642s 18ms/step - loss: 1.5598 - acc: 0.9217 - val_loss: 2.4590 - val_acc: 0.8501\n",
      "Epoch 9/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 10:13 - loss: 1.9131 - acc: 0.86 - ETA: 9:23 - loss: 1.9874 - acc: 0.8560 - ETA: 9:25 - loss: 1.8952 - acc: 0.862 - ETA: 9:03 - loss: 1.8237 - acc: 0.867 - ETA: 9:06 - loss: 1.7875 - acc: 0.862 - ETA: 8:51 - loss: 1.7607 - acc: 0.860 - ETA: 8:54 - loss: 1.7359 - acc: 0.861 - ETA: 8:40 - loss: 1.7169 - acc: 0.863 - ETA: 8:35 - loss: 1.7081 - acc: 0.862 - ETA: 8:23 - loss: 1.7030 - acc: 0.860 - ETA: 8:16 - loss: 1.6956 - acc: 0.862 - ETA: 8:05 - loss: 1.6926 - acc: 0.861 - ETA: 7:59 - loss: 1.6891 - acc: 0.860 - ETA: 7:49 - loss: 1.6852 - acc: 0.860 - ETA: 7:46 - loss: 1.6830 - acc: 0.859 - ETA: 7:38 - loss: 1.6806 - acc: 0.858 - ETA: 7:28 - loss: 1.6773 - acc: 0.857 - ETA: 7:21 - loss: 1.6738 - acc: 0.856 - ETA: 7:10 - loss: 1.6697 - acc: 0.856 - ETA: 7:04 - loss: 1.6657 - acc: 0.856 - ETA: 6:54 - loss: 1.6610 - acc: 0.856 - ETA: 6:52 - loss: 1.6568 - acc: 0.856 - ETA: 6:42 - loss: 1.6520 - acc: 0.856 - ETA: 6:35 - loss: 1.6477 - acc: 0.857 - ETA: 6:27 - loss: 1.6417 - acc: 0.858 - ETA: 6:17 - loss: 1.6387 - acc: 0.856 - ETA: 6:09 - loss: 1.6339 - acc: 0.856 - ETA: 5:59 - loss: 1.6319 - acc: 0.855 - ETA: 5:50 - loss: 1.6280 - acc: 0.855 - ETA: 5:41 - loss: 1.6256 - acc: 0.854 - ETA: 5:33 - loss: 1.6219 - acc: 0.855 - ETA: 5:24 - loss: 1.6206 - acc: 0.853 - ETA: 5:17 - loss: 1.6174 - acc: 0.853 - ETA: 5:07 - loss: 1.6153 - acc: 0.852 - ETA: 4:59 - loss: 1.6102 - acc: 0.853 - ETA: 4:50 - loss: 1.6061 - acc: 0.853 - ETA: 4:42 - loss: 1.6012 - acc: 0.854 - ETA: 4:33 - loss: 1.5972 - acc: 0.854 - ETA: 4:25 - loss: 1.5931 - acc: 0.854 - ETA: 4:15 - loss: 1.5896 - acc: 0.854 - ETA: 4:07 - loss: 1.5863 - acc: 0.854 - ETA: 3:59 - loss: 1.5825 - acc: 0.855 - ETA: 3:50 - loss: 1.5797 - acc: 0.855 - ETA: 3:42 - loss: 1.5764 - acc: 0.855 - ETA: 3:33 - loss: 1.5727 - acc: 0.855 - ETA: 3:24 - loss: 1.5694 - acc: 0.856 - ETA: 3:16 - loss: 1.5662 - acc: 0.856 - ETA: 3:07 - loss: 1.5643 - acc: 0.855 - ETA: 2:58 - loss: 1.5617 - acc: 0.856 - ETA: 2:50 - loss: 1.5590 - acc: 0.856 - ETA: 2:41 - loss: 1.5559 - acc: 0.856 - ETA: 2:33 - loss: 1.5534 - acc: 0.856 - ETA: 2:24 - loss: 1.5507 - acc: 0.856 - ETA: 2:16 - loss: 1.5480 - acc: 0.857 - ETA: 2:07 - loss: 1.5451 - acc: 0.856 - ETA: 1:59 - loss: 1.5418 - acc: 0.857 - ETA: 1:51 - loss: 1.5396 - acc: 0.857 - ETA: 1:42 - loss: 1.5355 - acc: 0.858 - ETA: 1:34 - loss: 1.5321 - acc: 0.858 - ETA: 1:25 - loss: 1.5296 - acc: 0.858 - ETA: 1:17 - loss: 1.5271 - acc: 0.859 - ETA: 1:08 - loss: 1.5244 - acc: 0.859 - ETA: 1:00 - loss: 1.5216 - acc: 0.859 - ETA: 51s - loss: 1.5190 - acc: 0.860 - ETA: 42s - loss: 1.5160 - acc: 0.86 - ETA: 34s - loss: 1.5133 - acc: 0.86 - ETA: 25s - loss: 1.5109 - acc: 0.86 - ETA: 17s - loss: 1.5090 - acc: 0.86 - ETA: 8s - loss: 1.5062 - acc: 0.8616 - 661s 19ms/step - loss: 1.5035 - acc: 0.8622 - val_loss: 1.4962 - val_acc: 0.8656\n",
      "Epoch 10/12\n",
      "35000/35000 [==============================] - ETA: 10:53 - loss: 1.3214 - acc: 0.89 - ETA: 10:27 - loss: 1.3104 - acc: 0.88 - ETA: 9:53 - loss: 1.3137 - acc: 0.8867 - ETA: 9:45 - loss: 1.3056 - acc: 0.885 - ETA: 9:22 - loss: 1.3266 - acc: 0.876 - ETA: 9:16 - loss: 1.3204 - acc: 0.876 - ETA: 9:02 - loss: 1.3165 - acc: 0.877 - ETA: 8:55 - loss: 1.3171 - acc: 0.881 - ETA: 8:40 - loss: 1.3187 - acc: 0.881 - ETA: 8:35 - loss: 1.3154 - acc: 0.883 - ETA: 8:22 - loss: 1.3157 - acc: 0.887 - ETA: 8:16 - loss: 1.3127 - acc: 0.889 - ETA: 8:05 - loss: 1.3095 - acc: 0.891 - ETA: 7:58 - loss: 1.3085 - acc: 0.892 - ETA: 7:56 - loss: 1.3048 - acc: 0.893 - ETA: 7:43 - loss: 1.3013 - acc: 0.893 - ETA: 7:36 - loss: 1.2990 - acc: 0.893 - ETA: 7:26 - loss: 1.2956 - acc: 0.894 - ETA: 7:20 - loss: 1.2921 - acc: 0.895 - ETA: 7:09 - loss: 1.2891 - acc: 0.896 - ETA: 7:02 - loss: 1.2850 - acc: 0.896 - ETA: 6:52 - loss: 1.2844 - acc: 0.897 - ETA: 6:45 - loss: 1.2847 - acc: 0.897 - ETA: 6:36 - loss: 1.2811 - acc: 0.898 - ETA: 6:28 - loss: 1.2838 - acc: 0.898 - ETA: 6:19 - loss: 1.2844 - acc: 0.898 - ETA: 6:11 - loss: 1.2842 - acc: 0.899 - ETA: 6:03 - loss: 1.2850 - acc: 0.898 - ETA: 5:54 - loss: 1.2832 - acc: 0.898 - ETA: 5:45 - loss: 1.2814 - acc: 0.899 - ETA: 5:36 - loss: 1.2809 - acc: 0.899 - ETA: 5:28 - loss: 1.2787 - acc: 0.900 - ETA: 5:18 - loss: 1.2764 - acc: 0.900 - ETA: 5:10 - loss: 1.2751 - acc: 0.901 - ETA: 5:02 - loss: 1.2727 - acc: 0.901 - ETA: 4:53 - loss: 1.2718 - acc: 0.901 - ETA: 4:44 - loss: 1.2704 - acc: 0.901 - ETA: 4:36 - loss: 1.2687 - acc: 0.902 - ETA: 4:28 - loss: 1.2662 - acc: 0.902 - ETA: 4:19 - loss: 1.2651 - acc: 0.902 - ETA: 4:10 - loss: 1.2626 - acc: 0.903 - ETA: 4:01 - loss: 1.2599 - acc: 0.903 - ETA: 3:53 - loss: 1.2582 - acc: 0.904 - ETA: 3:44 - loss: 1.2578 - acc: 0.904 - ETA: 3:36 - loss: 1.2571 - acc: 0.904 - ETA: 3:27 - loss: 1.2561 - acc: 0.904 - ETA: 3:18 - loss: 1.2539 - acc: 0.905 - ETA: 3:09 - loss: 1.2524 - acc: 0.906 - ETA: 3:02 - loss: 1.2511 - acc: 0.906 - ETA: 2:54 - loss: 1.2491 - acc: 0.907 - ETA: 2:45 - loss: 1.2473 - acc: 0.907 - ETA: 2:36 - loss: 1.2462 - acc: 0.907 - ETA: 2:28 - loss: 1.2445 - acc: 0.908 - ETA: 2:19 - loss: 1.2433 - acc: 0.908 - ETA: 2:10 - loss: 1.2437 - acc: 0.908 - ETA: 2:01 - loss: 1.2430 - acc: 0.908 - ETA: 1:52 - loss: 1.2421 - acc: 0.909 - ETA: 1:44 - loss: 1.2409 - acc: 0.909 - ETA: 1:35 - loss: 1.2423 - acc: 0.908 - ETA: 1:26 - loss: 1.2403 - acc: 0.908 - ETA: 1:18 - loss: 1.2394 - acc: 0.909 - ETA: 1:09 - loss: 1.2380 - acc: 0.909 - ETA: 1:01 - loss: 1.2359 - acc: 0.909 - ETA: 52s - loss: 1.2355 - acc: 0.909 - ETA: 43s - loss: 1.2335 - acc: 0.91 - ETA: 35s - loss: 1.2321 - acc: 0.91 - ETA: 26s - loss: 1.2308 - acc: 0.91 - ETA: 17s - loss: 1.2297 - acc: 0.91 - ETA: 8s - loss: 1.2278 - acc: 0.9118 - 670s 19ms/step - loss: 1.2267 - acc: 0.9121 - val_loss: 1.3900 - val_acc: 0.8832\n",
      "Epoch 11/12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000/35000 [==============================] - ETA: 10:13 - loss: 1.1284 - acc: 0.91 - ETA: 10:01 - loss: 1.1220 - acc: 0.91 - ETA: 9:59 - loss: 1.1313 - acc: 0.9047 - ETA: 10:02 - loss: 1.1127 - acc: 0.91 - ETA: 9:40 - loss: 1.1296 - acc: 0.9144 - ETA: 9:29 - loss: 1.1238 - acc: 0.920 - ETA: 9:22 - loss: 1.1194 - acc: 0.920 - ETA: 9:13 - loss: 1.1157 - acc: 0.921 - ETA: 9:00 - loss: 1.1178 - acc: 0.920 - ETA: 8:54 - loss: 1.1179 - acc: 0.921 - ETA: 8:39 - loss: 1.1142 - acc: 0.921 - ETA: 8:32 - loss: 1.1162 - acc: 0.920 - ETA: 8:21 - loss: 1.1135 - acc: 0.918 - ETA: 8:13 - loss: 1.1145 - acc: 0.917 - ETA: 8:05 - loss: 1.1112 - acc: 0.915 - ETA: 7:57 - loss: 1.1127 - acc: 0.913 - ETA: 7:49 - loss: 1.1121 - acc: 0.912 - ETA: 7:37 - loss: 1.1110 - acc: 0.911 - ETA: 7:31 - loss: 1.1104 - acc: 0.910 - ETA: 7:20 - loss: 1.1091 - acc: 0.909 - ETA: 7:14 - loss: 1.1064 - acc: 0.910 - ETA: 7:04 - loss: 1.1050 - acc: 0.909 - ETA: 6:56 - loss: 1.1021 - acc: 0.907 - ETA: 6:48 - loss: 1.1011 - acc: 0.908 - ETA: 6:40 - loss: 1.1029 - acc: 0.908 - ETA: 6:32 - loss: 1.1009 - acc: 0.907 - ETA: 6:22 - loss: 1.0979 - acc: 0.907 - ETA: 6:14 - loss: 1.0957 - acc: 0.906 - ETA: 6:04 - loss: 1.0948 - acc: 0.905 - ETA: 5:55 - loss: 1.0919 - acc: 0.904 - ETA: 5:46 - loss: 1.0914 - acc: 0.904 - ETA: 5:37 - loss: 1.0894 - acc: 0.904 - ETA: 5:29 - loss: 1.0876 - acc: 0.903 - ETA: 5:19 - loss: 1.0858 - acc: 0.903 - ETA: 5:10 - loss: 1.0859 - acc: 0.903 - ETA: 5:01 - loss: 1.0867 - acc: 0.902 - ETA: 4:53 - loss: 1.0870 - acc: 0.902 - ETA: 4:43 - loss: 1.0870 - acc: 0.902 - ETA: 4:34 - loss: 1.0864 - acc: 0.902 - ETA: 4:25 - loss: 1.0847 - acc: 0.901 - ETA: 4:18 - loss: 1.0853 - acc: 0.901 - ETA: 4:10 - loss: 1.0841 - acc: 0.900 - ETA: 4:00 - loss: 1.0827 - acc: 0.900 - ETA: 3:52 - loss: 1.0815 - acc: 0.900 - ETA: 3:42 - loss: 1.0797 - acc: 0.899 - ETA: 3:33 - loss: 1.0795 - acc: 0.899 - ETA: 3:24 - loss: 1.0768 - acc: 0.899 - ETA: 3:16 - loss: 1.0761 - acc: 0.898 - ETA: 3:07 - loss: 1.0755 - acc: 0.898 - ETA: 2:58 - loss: 1.0745 - acc: 0.898 - ETA: 2:49 - loss: 1.0739 - acc: 0.898 - ETA: 2:40 - loss: 1.0721 - acc: 0.898 - ETA: 2:31 - loss: 1.0724 - acc: 0.897 - ETA: 2:22 - loss: 1.0707 - acc: 0.897 - ETA: 2:14 - loss: 1.0698 - acc: 0.897 - ETA: 2:05 - loss: 1.0698 - acc: 0.896 - ETA: 1:56 - loss: 1.0693 - acc: 0.896 - ETA: 1:47 - loss: 1.0676 - acc: 0.896 - ETA: 1:38 - loss: 1.0690 - acc: 0.896 - ETA: 1:29 - loss: 1.0688 - acc: 0.895 - ETA: 1:20 - loss: 1.0675 - acc: 0.895 - ETA: 1:11 - loss: 1.0659 - acc: 0.895 - ETA: 1:02 - loss: 1.0653 - acc: 0.896 - ETA: 53s - loss: 1.0646 - acc: 0.896 - ETA: 44s - loss: 1.0630 - acc: 0.89 - ETA: 35s - loss: 1.0616 - acc: 0.89 - ETA: 26s - loss: 1.0604 - acc: 0.89 - ETA: 17s - loss: 1.0598 - acc: 0.89 - ETA: 8s - loss: 1.0583 - acc: 0.8988 - 682s 19ms/step - loss: 1.0574 - acc: 0.8994 - val_loss: 1.1932 - val_acc: 0.8835\n",
      "Epoch 12/12\n",
      "35000/35000 [==============================] - ETA: 10:02 - loss: 0.9522 - acc: 0.93 - ETA: 10:14 - loss: 0.9659 - acc: 0.92 - ETA: 10:21 - loss: 0.9914 - acc: 0.91 - ETA: 9:56 - loss: 1.0134 - acc: 0.9095 - ETA: 9:45 - loss: 1.0327 - acc: 0.902 - ETA: 9:29 - loss: 1.0386 - acc: 0.901 - ETA: 9:28 - loss: 1.0392 - acc: 0.904 - ETA: 9:13 - loss: 1.0375 - acc: 0.906 - ETA: 9:05 - loss: 1.0346 - acc: 0.908 - ETA: 8:54 - loss: 1.0317 - acc: 0.908 - ETA: 8:46 - loss: 1.0255 - acc: 0.911 - ETA: 8:39 - loss: 1.0221 - acc: 0.913 - ETA: 8:28 - loss: 1.0157 - acc: 0.914 - ETA: 8:21 - loss: 1.0124 - acc: 0.915 - ETA: 8:09 - loss: 1.0102 - acc: 0.915 - ETA: 8:03 - loss: 1.0059 - acc: 0.914 - ETA: 7:52 - loss: 1.0038 - acc: 0.913 - ETA: 7:44 - loss: 1.0054 - acc: 0.911 - ETA: 7:33 - loss: 1.0030 - acc: 0.912 - ETA: 7:26 - loss: 1.0030 - acc: 0.910 - ETA: 7:18 - loss: 1.0013 - acc: 0.910 - ETA: 7:08 - loss: 1.0035 - acc: 0.909 - ETA: 7:00 - loss: 1.0049 - acc: 0.908 - ETA: 6:50 - loss: 1.0040 - acc: 0.908 - ETA: 6:43 - loss: 1.0038 - acc: 0.909 - ETA: 6:33 - loss: 1.0038 - acc: 0.909 - ETA: 6:25 - loss: 1.0037 - acc: 0.910 - ETA: 6:17 - loss: 1.0033 - acc: 0.910 - ETA: 6:07 - loss: 1.0025 - acc: 0.911 - ETA: 5:59 - loss: 1.0013 - acc: 0.911 - ETA: 5:52 - loss: 1.0018 - acc: 0.911 - ETA: 5:45 - loss: 1.0024 - acc: 0.911 - ETA: 5:37 - loss: 1.0023 - acc: 0.911 - ETA: 5:28 - loss: 1.0004 - acc: 0.913 - ETA: 5:20 - loss: 0.9984 - acc: 0.913 - ETA: 5:10 - loss: 0.9970 - acc: 0.914 - ETA: 5:02 - loss: 0.9955 - acc: 0.914 - ETA: 4:54 - loss: 0.9936 - acc: 0.915 - ETA: 4:44 - loss: 0.9919 - acc: 0.915 - ETA: 4:35 - loss: 0.9907 - acc: 0.916 - ETA: 4:26 - loss: 0.9905 - acc: 0.916 - ETA: 4:17 - loss: 0.9922 - acc: 0.916 - ETA: 4:08 - loss: 1.0009 - acc: 0.909 - ETA: 3:59 - loss: 1.0192 - acc: 0.895 - ETA: 3:50 - loss: 1.0480 - acc: 0.879 - ETA: 3:40 - loss: 1.0788 - acc: 0.863 - ETA: 3:31 - loss: 1.1114 - acc: 0.847 - ETA: 3:23 - loss: 1.1364 - acc: 0.833 - ETA: 3:13 - loss: 1.1567 - acc: 0.819 - ETA: 3:04 - loss: 1.1722 - acc: 0.805 - ETA: 2:55 - loss: 1.1839 - acc: 0.792 - ETA: 2:46 - loss: 1.1925 - acc: 0.781 - ETA: 2:36 - loss: 1.1989 - acc: 0.773 - ETA: 2:27 - loss: 1.2033 - acc: 0.769 - ETA: 2:18 - loss: 1.2061 - acc: 0.769 - ETA: 2:09 - loss: 1.2077 - acc: 0.770 - ETA: 2:00 - loss: 1.2080 - acc: 0.772 - ETA: 1:51 - loss: 1.2076 - acc: 0.773 - ETA: 1:41 - loss: 1.2073 - acc: 0.774 - ETA: 1:32 - loss: 1.2073 - acc: 0.776 - ETA: 1:23 - loss: 1.2146 - acc: 0.777 - ETA: 1:14 - loss: 1.2228 - acc: 0.779 - ETA: 1:05 - loss: 1.2272 - acc: 0.780 - ETA: 56s - loss: 1.2306 - acc: 0.781 - ETA: 46s - loss: 1.2309 - acc: 0.78 - ETA: 37s - loss: 1.2290 - acc: 0.78 - ETA: 28s - loss: 1.2282 - acc: 0.78 - ETA: 18s - loss: 1.2268 - acc: 0.78 - ETA: 9s - loss: 1.2248 - acc: 0.7864 - 719s 21ms/step - loss: 1.2235 - acc: 0.7873 - val_loss: 1.1492 - val_acc: 0.8437\n"
     ]
    }
   ],
   "source": [
    "model_2 = model_2.fit(x_train, np.array(y_train), batch_size = batch_size, epochs = epochs, verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Plot train and cross validation error\n",
    "plot_train_cv_loss(model_2, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+--------+--------------+---------------------+-----------+-----------+---------------+\n",
      "| Model No. | Architecture | Epochs | Dropout rate | Activation function | Optimizer | Test Loss | Test Accuracy |\n",
      "+-----------+--------------+--------+--------------+---------------------+-----------+-----------+---------------+\n",
      "|  Model 1  | 1_Layer LSTM |   12   |     0.2      |       Sigmoid       |    Adam   |   0.3256  |     0.8846    |\n",
      "|  Model 2  | 2 Layer LSTM |   12   |     0.2      |         Relu        |    Adam   |   1.141   |     0.8437    |\n",
      "+-----------+--------------+--------+--------------+---------------------+-----------+-----------+---------------+\n"
     ]
    }
   ],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "x = PrettyTable()\n",
    "\n",
    "x.field_names = [\"Model No.\",\"Architecture\",\"Epochs\",\"Dropout rate\",\"Activation function\",\"Optimizer\",\"Test Loss\",\"Test Accuracy\"]\n",
    "\n",
    "x.add_row([\"Model 1\",\"1_Layer LSTM\",\"12\", \"0.2\",\"Sigmoid\",\"Adam\",0.3256,0.8846])\n",
    "\n",
    "x.add_row([\"Model 2\",\"2 Layer LSTM\",\"12\", \"0.2\",\"Relu\",\"Adam\",1.141,0.8437])\n",
    "\n",
    "\n",
    "print(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
